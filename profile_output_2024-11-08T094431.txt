Timer unit: 1e-09 s

Total time: 2.83906 s
File: /Users/home/school/thesis/TESTING/math_seq2tree/src/train_and_evaluate.py
Function: evaluate_tree at line 491

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   491                                           @line_profiler.profile
   492                                           def evaluate_tree(input_batch, input_length, generate_nums, encoder, predict, generate, merge, output_lang, num_pos,
   493                                                             beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):
   494                                           
   495        30     589000.0  19633.3      0.0      seq_mask = torch.ByteTensor(1, input_length).fill_(0)
   496                                               # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)
   497        30     402000.0  13400.0      0.0      input_var = torch.LongTensor(input_batch).unsqueeze(1)
   498                                           
   499        30      71000.0   2366.7      0.0      num_mask = torch.ByteTensor(1, len(num_pos) + len(generate_nums)).fill_(0)
   500                                           
   501                                               # Set to not-training mode to disable dropout
   502        30    1171000.0  39033.3      0.0      encoder.eval()
   503        30    1506000.0  50200.0      0.1      predict.eval()
   504        30     748000.0  24933.3      0.0      generate.eval()
   505        30     424000.0  14133.3      0.0      merge.eval()
   506                                           
   507        30    1606000.0  53533.3      0.1      padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)
   508                                           
   509        30      11000.0    366.7      0.0      batch_size = 1
   510                                           
   511        30       4000.0    133.3      0.0      if USE_CUDA:
   512                                                   input_var = input_var.cuda()
   513                                                   seq_mask = seq_mask.cuda()
   514                                                   padding_hidden = padding_hidden.cuda()
   515                                                   num_mask = num_mask.cuda()
   516                                               # Run words through encoder
   517                                           
   518        30  300691000.0    1e+07     10.6      encoder_outputs, problem_output = encoder(input_var, [input_length])
   519                                           
   520                                               # Prepare input and output variables
   521        30     513000.0  17100.0      0.0      node_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]
   522                                           
   523        30      15000.0    500.0      0.0      num_size = len(num_pos)
   524        60    5318000.0  88633.3      0.2      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,
   525        30       9000.0    300.0      0.0                                                                encoder.hidden_size)
   526        30      26000.0    866.7      0.0      num_start = output_lang.num_start
   527                                               # B x P x N
   528        30      32000.0   1066.7      0.0      embeddings_stacks = [[] for _ in range(batch_size)]
   529        30      21000.0    700.0      0.0      left_childs = [None for _ in range(batch_size)]
   530                                           
   531                                               # evaulation uses beam search
   532                                               # key is how the beams are compared
   533        30     404000.0  13466.7      0.0      beams = [TreeBeam(0.0, node_stacks, embeddings_stacks, left_childs, [])]
   534                                           
   535       355     123000.0    346.5      0.0      for t in range(max_length):
   536       350   38984000.0 111382.9      1.4          current_beams = []
   537      1980     848000.0    428.3      0.0          while len(beams) > 0:
   538      1630     886000.0    543.6      0.0              b = beams.pop()
   539      1630     625000.0    383.4      0.0              if len(b.node_stack[0]) == 0:
   540       493     116000.0    235.3      0.0                  current_beams.append(b)
   541       493      81000.0    164.3      0.0                  continue
   542                                                       # left_childs = torch.stack(b.left_childs)
   543      1137     196000.0    172.4      0.0              left_childs = b.left_childs
   544                                           
   545      2274 1042794000.0 458572.6     36.7              num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(
   546      1137     204000.0    179.4      0.0                  b.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden,
   547      1137     141000.0    124.0      0.0                  seq_mask, num_mask)
   548                                           
   549                                                       # leaf = p_leaf[:, 0].unsqueeze(1)
   550                                                       # repeat_dims = [1] * leaf.dim()
   551                                                       # repeat_dims[1] = op.size(1)
   552                                                       # leaf = leaf.repeat(*repeat_dims)
   553                                                       #
   554                                                       # non_leaf = p_leaf[:, 1].unsqueeze(1)
   555                                                       # repeat_dims = [1] * non_leaf.dim()
   556                                                       # repeat_dims[1] = num_score.size(1)
   557                                                       # non_leaf = non_leaf.repeat(*repeat_dims)
   558                                                       #
   559                                                       # p_leaf = torch.cat((leaf, non_leaf), dim=1)
   560      1137   14500000.0  12752.9      0.5              out_score = nn.functional.log_softmax(torch.cat((op, num_score), dim=1), dim=1)
   561                                           
   562                                                       # out_score = p_leaf * out_score
   563                                           
   564                                                       # topv:
   565                                                       #   largest elements in the out_score
   566                                                       # topi:
   567                                                       #   indexes of the largest elements 
   568      1137   10892000.0   9579.6      0.4              topv, topi = out_score.topk(beam_size)
   569                                           
   570                                                       # is_leaf = int(topi[0])
   571                                                       # if is_leaf:
   572                                                       #     topv, topi = op.topk(1)
   573                                                       #     out_token = int(topi[0])
   574                                                       # else:
   575                                                       #     topv, topi = num_score.topk(1)
   576                                                       #     out_token = int(topi[0]) + num_start
   577                                           
   578                                                       # for the largest element, and its index
   579      6822   52173000.0   7647.8      1.8              for tv, ti in zip(topv.split(1, dim=1), topi.split(1, dim=1)):
   580      5685   26376000.0   4639.6      0.9                  current_node_stack = copy_list(b.node_stack)
   581      5685     816000.0    143.5      0.0                  current_left_childs = []
   582      5685   23047000.0   4054.0      0.8                  current_embeddings_stacks = copy_list(b.embedding_stack)
   583      5685   81683000.0  14368.2      2.9                  current_out = copy.deepcopy(b.out)
   584                                           
   585                                                           # the predicted token is that of the highest score relation
   586      5685    6031000.0   1060.9      0.2                  out_token = int(ti)
   587                                                           # save token 
   588      5685    1228000.0    216.0      0.0                  current_out.append(out_token)
   589                                           
   590      5685    2130000.0    374.7      0.1                  node = current_node_stack[0].pop()
   591                                           
   592                                                           # if the predicted token is an operator
   593      5685     858000.0    150.9      0.0                  if out_token < num_start:
   594                                                               # this is the token to generate l and r from
   595      2738   15443000.0   5640.2      0.5                      generate_input = torch.LongTensor([out_token])
   596      2738     481000.0    175.7      0.0                      if USE_CUDA:
   597                                                                   generate_input = generate_input.cuda()
   598                                                               # get the left and right children and current label
   599      2738  872645000.0 318716.2     30.7                      left_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)
   600                                           
   601      2738    5355000.0   1955.8      0.2                      current_node_stack[0].append(TreeNode(right_child))
   602      2738    2571000.0    939.0      0.1                      current_node_stack[0].append(TreeNode(left_child, left_flag=True))
   603                                           
   604      2738   20062000.0   7327.2      0.7                      current_embeddings_stacks[0].append(TreeEmbedding(node_label[0].unsqueeze(0), False))
   605                                                           else:
   606                                                               # predicted token is a number
   607                                                               # get the token embedding - embedding of either the generate num or copy num
   608      2947   18887000.0   6408.9      0.7                      current_num = current_nums_embeddings[0, out_token - num_start].unsqueeze(0)
   609                                                               
   610                                                               # if we are a right node (there is a left node and operator)
   611      3860    1741000.0    451.0      0.1                      while len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:
   612       913     338000.0    370.2      0.0                          sub_stree = current_embeddings_stacks[0].pop()
   613       913     312000.0    341.7      0.0                          op = current_embeddings_stacks[0].pop()
   614       913  111035000.0 121615.6      3.9                          current_num = merge(op.embedding, sub_stree.embedding, current_num)
   615                                                               # save node (or subtree) to the embeddings list
   616      2947    2378000.0    806.9      0.1                      current_embeddings_stacks[0].append(TreeEmbedding(current_num, True))
   617      5685    2853000.0    501.8      0.1                  if len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:
   618      2947     926000.0    314.2      0.0                      current_left_childs.append(current_embeddings_stacks[0][-1].embedding)
   619                                                           else:
   620      2738     782000.0    285.6      0.0                      current_left_childs.append(None)
   621                                                           # the beam "score" is the sum of the associations 
   622     11370  156801000.0  13790.8      5.5                  current_beams.append(TreeBeam(b.score+float(tv), current_node_stack, current_embeddings_stacks,
   623      5685    4379000.0    770.3      0.2                                                current_left_childs, current_out))
   624                                                   # order beam by highest to lowest
   625       350    2392000.0   6834.3      0.1          beams = sorted(current_beams, key=lambda x: x.score, reverse=True)
   626       350     255000.0    728.6      0.0          beams = beams[:beam_size]
   627       350      44000.0    125.7      0.0          flag = True
   628      2100     411000.0    195.7      0.0          for b in beams:
   629      1750     479000.0    273.7      0.0              if len(b.node_stack[0]) != 0:
   630      1122     121000.0    107.8      0.0                  flag = False
   631       350      68000.0    194.3      0.0          if flag:
   632        25       6000.0    240.0      0.0              break
   633                                           
   634        30       4000.0    133.3      0.0      return beams[0].out

Total time: 4.65491 s
File: /Users/home/school/thesis/TESTING/math_seq2tree/src/train_and_evaluate.py
Function: train_tree at line 257

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   257                                           @line_profiler.profile
   258                                           def train_tree(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, generate_nums,
   259                                                          encoder, predict, generate, merge, encoder_optimizer, predict_optimizer, generate_optimizer,
   260                                                          merge_optimizer, output_lang, num_pos, english=False):
   261                                               # input_batch: padded inputs
   262                                               # input_length: length of the inputs (without padding)
   263                                               # target_batch: padded outputs
   264                                               # target_length: length of the outputs (without padding)
   265                                               # num_stack_batch: the corresponding nums lists
   266                                               # num_size_batch: number of numbers from the input text
   267                                               # generate_nums: numbers to generate
   268                                           
   269                                               # num_pos: positions of the numbers lists
   270                                           
   271                                           
   272                                               # sequence mask for attention
   273                                               # 0s where in input, 1s where not in input
   274        15       7000.0    466.7      0.0      seq_mask = []
   275        15      35000.0   2333.3      0.0      max_len = max(input_length)
   276       135      18000.0    133.3      0.0      for i in input_length:
   277       120     459000.0   3825.0      0.0          seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])
   278        15    1738000.0 115866.7      0.0      seq_mask = torch.ByteTensor(seq_mask)
   279                                           
   280                                               # number mask 
   281                                               # 0s where the numbers are from input, 1s where not in input
   282        15       3000.0    200.0      0.0      num_mask = []
   283        15      14000.0    933.3      0.0      max_num_size = max(num_size_batch) + len(generate_nums)
   284       135      14000.0    103.7      0.0      for i in num_size_batch:
   285       120      22000.0    183.3      0.0          d = i + len(generate_nums)
   286       120      68000.0    566.7      0.0          num_mask.append([0] * d + [1] * (max_num_size - d))
   287        15      84000.0   5600.0      0.0      num_mask = torch.ByteTensor(num_mask)
   288                                           
   289        15      15000.0   1000.0      0.0      unk = output_lang.word2index["UNK"]
   290                                           
   291                                               # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)
   292        15     756000.0  50400.0      0.0      input_var = torch.LongTensor(input_batch).transpose(0, 1)
   293        15     120000.0   8000.0      0.0      target = torch.LongTensor(target_batch).transpose(0, 1)
   294                                           
   295        15    1127000.0  75133.3      0.0      padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)
   296        15       6000.0    400.0      0.0      batch_size = len(input_length)
   297                                           
   298        15     557000.0  37133.3      0.0      encoder.train()
   299        15     694000.0  46266.7      0.0      predict.train()
   300        15     348000.0  23200.0      0.0      generate.train()
   301        15     205000.0  13666.7      0.0      merge.train()
   302                                           
   303        15       3000.0    200.0      0.0      if USE_CUDA:
   304                                                   input_var = input_var.cuda()
   305                                                   seq_mask = seq_mask.cuda()
   306                                                   padding_hidden = padding_hidden.cuda()
   307                                                   num_mask = num_mask.cuda()
   308                                           
   309                                               # Zero gradients of both optimizers
   310        15   11872000.0 791466.7      0.3      encoder_optimizer.zero_grad()
   311        15    5869000.0 391266.7      0.1      predict_optimizer.zero_grad()
   312        15    5160000.0 344000.0      0.1      generate_optimizer.zero_grad()
   313        15    3464000.0 230933.3      0.1      merge_optimizer.zero_grad()
   314                                               # Run words through encoder
   315                                           
   316                                               # embedding + dropout layer
   317                                               # encoder_outputs: num_batches x 512 q_0 vector
   318                                               # problem_output: max_length x num_batches x hidden_size
   319        15  743143000.0    5e+07     16.0      encoder_outputs, problem_output = encoder(input_var, input_length)
   320                                               # Prepare input and output variables
   321                                               
   322                                               # make a TreeNode for each token
   323                                               # node just has embedding and a left flag? 
   324        15     817000.0  54466.7      0.0      tempSplit = problem_output.split(1, dim=0)
   325                                               # problem_output is q_0 for each token in equation, so use last one
   326        15     266000.0  17733.3      0.0      node_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]
   327                                           
   328        15      29000.0   1933.3      0.0      max_target_length = max(target_length)
   329                                           
   330        15       4000.0    266.7      0.0      all_node_outputs = []
   331                                               # all_leafs = []
   332                                           
   333                                               # array of len of numbers that must be copied from the input text
   334        15      37000.0   2466.7      0.0      copy_num_len = [len(_) for _ in num_pos]
   335                                               # max nums to copy
   336        15       8000.0    533.3      0.0      num_size = max(copy_num_len)
   337                                           
   338                                               # num_batches x num_size x hidden_size that correspond to the embeddings of the numbers
   339        30   20171000.0 672366.7      0.4      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size,
   340        15       7000.0    466.7      0.0                                                                encoder.hidden_size)
   341                                           
   342                                           
   343                                               # index in the language where the special (operators) tokens end and input/output text begins
   344        15      15000.0   1000.0      0.0      num_start = output_lang.num_start
   345                                               # 
   346        15      53000.0   3533.3      0.0      embeddings_stacks = [[] for _ in range(batch_size)]
   347                                               # 
   348        15      26000.0   1733.3      0.0      left_childs = [None for _ in range(batch_size)]
   349       154      45000.0    292.2      0.0      for t in range(max_target_length):
   350                                           
   351                                                   # predict gets the encodings and embeddings for the current node 
   352                                                   #   num_score: batch_size x num_length
   353                                                   #       likliehood prediction of each number
   354                                                   #   op: batch_size x num_ops
   355                                                   #       likliehood of the operator tokens
   356                                                   #   current_embeddings: batch_size x 1 x hidden_size
   357                                                   #       goal vector (q) for the current node 
   358                                                   #   current_context: batch_size x 1 x hidden_size
   359                                                   #       context vector (c) for the subtree
   360                                                   #   embedding_weight: batch_size x num_length x hidden_size
   361                                                   #       embeddings of the generate and copy numbers
   362                                           
   363       278  432102000.0    2e+06      9.3          num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(
   364       139      44000.0    316.5      0.0              node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask)
   365                                           
   366                                           
   367                                                   # this is mainly what we want to train
   368       139     757000.0   5446.0      0.0          outputs = torch.cat((op, num_score), 1)
   369       139      94000.0    676.3      0.0          all_node_outputs.append(outputs)
   370                                           
   371                                                   # target[t] is the equation character at index t for each batch
   372                                                   #    target[t] = 1 x num_batches
   373                                                   # outputs is the strength of operators or a number token
   374                                                   # num_stack_batch is the cooresponding num lists
   375                                                   # num_start is where non-operators begin
   376                                                   # unk is unknown token
   377                                                   # returns
   378                                                   #   for position t in each equation
   379                                                   #       target_t: actual equation value
   380                                                   #       generate_input: equation value if its an operator
   381       139    8285000.0  59604.3      0.2          target_t, generate_input = generate_tree_input(target[t].tolist(), outputs, nums_stack_batch, num_start, unk)
   382       139     730000.0   5251.8      0.0          target[t] = target_t
   383       139      44000.0    316.5      0.0          if USE_CUDA:
   384                                                       generate_input = generate_input.cuda()
   385                                           
   386                                                   # takes:
   387                                                   #     generate a left and right child node with a label
   388                                                   #     current_embeddings: q : batch_size x 1 x hidden_dim
   389                                                   #     generate_input: [operator tokens at position t]
   390                                                   #     current_context: c : batch_size x 1 x hidden_dim
   391                                                   # returns
   392                                                   #     l_child: batch_size x hidden_dim
   393                                                   #          hidden state h_l:
   394                                                   #     r_child: batch_size x hidden_dim
   395                                                   #          hidden state h_r:
   396                                                   #     node_label_ : batch_size x embedding_size 
   397                                                   #          basically the context vector (c)
   398                                                   # the node generation takes the first half of equations (10) and (11) 
   399       139  142550000.0    1e+06      3.1          left_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)
   400       139     103000.0    741.0      0.0          left_childs = []
   401      1470    8675000.0   5901.4      0.2          for idx, l, r, node_stack, i, o in zip(range(batch_size), left_child.split(1), right_child.split(1),
   402       139     821000.0   5906.5      0.0                                                 node_stacks, target[t].tolist(), embeddings_stacks):
   403      1192    1158000.0    971.5      0.0              current_token = output_lang.ids_to_tokens([i])
   404      1192   18073000.0  15161.9      0.4              current_equation = output_lang.ids_to_tokens(target.transpose(0,1)[idx])
   405                                                       #print("at token", current_token, "in", current_equation)
   406                                                       #print("current node_stack length", len(node_stack))
   407                                                       # for 
   408                                                       #   batch_num
   409                                                       #   the left child: h_l 
   410                                                       #   the right child: h_r
   411      1192     319000.0    267.6      0.0              if len(node_stack) != 0:
   412       730     943000.0   1291.8      0.0                  node = node_stack.pop()
   413                                                           #print("removed last from node_stack, now", len(node_stack), "elems")
   414                                                       else:
   415       462     104000.0    225.1      0.0                  left_childs.append(None)
   416       462      45000.0     97.4      0.0                  continue
   417                                           
   418                                                       # i is the num in language of where that specific language token is
   419                                                       # if i is an operator
   420       730     123000.0    168.5      0.0              if i < num_start:
   421                                                           #print(current_token, "is an operator, making a left and right node")
   422                                                           # make a left and right tree node
   423       305     302000.0    990.2      0.0                  node_stack.append(TreeNode(r))
   424       305     238000.0    780.3      0.0                  node_stack.append(TreeNode(l, left_flag=True))
   425                                                           # save the embedding of the operator 
   426                                                           # terminal means a leaf node
   427       305    1195000.0   3918.0      0.0                  o.append(TreeEmbedding(node_label[idx].unsqueeze(0), False))
   428                                                           #print("saving node embedding to o (non terminal node), and r, and l to node_stack. o now of size", len(o), "node_stack of size", len(node_stack))
   429                                                       else:
   430                                                           #print(current_token, "is not an operator")
   431                                                           # otherwise its either a number from the input equation or a copy number
   432                                                           # we have a list (o) of the current nodes in the tree
   433                                                           # if we have a leaf node at the top of the stack, get it.
   434                                                           # next element in the stack must be an operator, so get it 
   435                                                           # and combine the new node, operator, and other element
   436                                           
   437                                                           # current_nums_embedding: batch_size x num_length x hidden_size
   438                                                           # current_num = num_embedding of the number selected
   439       425    2249000.0   5291.8      0.0                  current_num = current_nums_embeddings[idx, i - num_start].unsqueeze(0)
   440                                                           # while there are tokens in the embedding stack and the last element IS a leaf node
   441       730     407000.0    557.5      0.0                  while len(o) > 0 and o[-1].terminal:
   442                                                               #print("terminal element in o, getting terminal element and operator, and merging")
   443                                                               # get the two elements from it
   444       305     446000.0   1462.3      0.0                      sub_stree = o.pop()
   445       305     245000.0    803.3      0.0                      op = o.pop()
   446                                                               # contains equation (13)
   447                                                               # this combines a left and right tree along with a node
   448       305   48968000.0 160550.8      1.1                      current_num = merge(op.embedding, sub_stree.embedding, current_num)
   449                                                               #print('merged. o now of size', len(o))
   450                                                           # then re-add the node back to the stack
   451                                                           #print("adding current_num to o (terminal node)")
   452       425     480000.0   1129.4      0.0                  o.append(TreeEmbedding(current_num, True))
   453       730     253000.0    346.6      0.0              if len(o) > 0 and o[-1].terminal:
   454                                                           #print("terminal element in o, adding to left child")
   455                                                           # left_childs is a running vector of the sub tree embeddings "t" 
   456                                                           # need this for generation of the right q
   457       425     156000.0    367.1      0.0                  left_childs.append(o[-1].embedding)
   458                                                       else:
   459       305      70000.0    229.5      0.0                  left_childs.append(None)
   460                                           
   461                                               # all_leafs = torch.stack(all_leafs, dim=1)  # B x S x 2
   462                                               
   463                                               # all_node_outputs:  for each token in the equation:
   464                                               #   the current scoring of nums for each batch
   465                                               # 
   466                                               # transform to 
   467                                               # all_node_outputs2: for each batch:
   468                                               #   the current scoring of nums for each token in equation
   469                                               # = batch_size x max_len x num_nums
   470        15     164000.0  10933.3      0.0      all_node_outputs2 = torch.stack(all_node_outputs, dim=1)  # B x S x N
   471                                           
   472        15      37000.0   2466.7      0.0      target = target.transpose(0, 1).contiguous()
   473        15       6000.0    400.0      0.0      if USE_CUDA:
   474                                                   # all_leafs = all_leafs.cuda()
   475                                                   all_node_outputs2 = all_node_outputs2.cuda()
   476                                                   target = target.cuda()
   477                                           
   478                                               # for batch in target:
   479                                               #     print([output_lang.index2word[_] for _ in batch])
   480                                               #print('done equation')
   481        15    6813000.0 454200.0      0.1      loss = masked_cross_entropy(all_node_outputs2, target, target_length)
   482        15 2796666000.0    2e+08     60.1      loss.backward()
   483                                           
   484                                               # Update parameters with optimizers
   485        15  208119000.0    1e+07      4.5      encoder_optimizer.step()
   486        15   82457000.0    5e+06      1.8      predict_optimizer.step()
   487        15   63370000.0    4e+06      1.4      generate_optimizer.step()
   488        15   30001000.0    2e+06      0.6      merge_optimizer.step()
   489        15      17000.0   1133.3      0.0      return loss.item() 

  2.84 seconds - /Users/home/school/thesis/TESTING/math_seq2tree/src/train_and_evaluate.py:491 - evaluate_tree
  4.65 seconds - /Users/home/school/thesis/TESTING/math_seq2tree/src/train_and_evaluate.py:257 - train_tree
