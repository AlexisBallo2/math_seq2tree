Timer unit: 1e-09 s

Total time: 5.55816 s
File: /Users/home/school/thesis/math_seq2tree/src/models.py
Function: forward at line 280

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   280                                               @line_profiler.profile  
   281                                               def forward(self, node_stacks, left_childs, encoder_outputs, num_pades, padding_hidden, seq_mask, mask_nums, var_mask, xs):
   282                                                   # node_stacks: [TreeNodes] for each node containing the hidden state for the node
   283                                                   # left_childs: [] of 
   284                                                   # encoder_outputs: token embeddings: max_len x num_batches x hidden state 
   285                                                   # num_pads:  embeddings of the numbers: num_batches x num_size x hidden_size 
   286                                                   # padding_hidden: 0s of hidden_size
   287                                                   # seq_mask: num_batches x seq_len 
   288                                                   # mask_nums: 0s where the num in the nums emb come from input text. 
   289                                                   # this aligns with the num_pades. num_batches x num_size
   290                                           
   291                                                   # let num_length = 2 + len(numbers)
   292                                           
   293      3144    1850000.0    588.4      0.0          current_embeddings1 = []
   294                                           
   295                                                   # for each stack of tokens 
   296                                                   # 2 batches = 2 stacks
   297      7368    2556000.0    346.9      0.0          for st in node_stacks:
   298                                                       # not sure why would be zero. it's initialized w/ single num each 
   299                                                       # if it is zero, let that token's embedding be the zero embedding
   300      4224    1945000.0    460.5      0.0              if len(st) == 0:
   301       908     205000.0    225.8      0.0                  current_embeddings1.append(padding_hidden)
   302                                                       else:
   303                                                           # use embedding from the last node in the stack
   304      3316    1003000.0    302.5      0.0                  current_node = st[-1]
   305      3316    1461000.0    440.6      0.0                  current_embeddings1.append(current_node.embedding)
   306                                                   
   307                                                   # current_embeddings1 = the embedding of the last node in the stack
   308                                           
   309      3144    1103000.0    350.8      0.0          current_node_temp = []
   310      7368    6119000.0    830.5      0.1          for l, c in zip(left_childs, current_embeddings1):
   311                                                       # l = left child
   312                                                       # c = embedding of parent node
   313                                                       # second half of equation (10)
   314      4224    1038000.0    245.7      0.0              if l is None:
   315                                                           # if not left child (this is a leaf)
   316                                                           # nodes context vector 1 x hidden_dim
   317      3177   43781000.0  13780.6      0.8                  c = self.dropout(c)
   318      3177  214116000.0  67395.7      3.9                  g = torch.tanh(self.concat_l(c))
   319      3177  203984000.0  64206.5      3.7                  t = torch.sigmoid(self.concat_lg(c))
   320      3177   11232000.0   3535.4      0.2                  current_node_temp.append(g * t)
   321                                                       else:
   322                                                           # second half of equation (11)
   323      1047    9313000.0   8894.9      0.2                  ld = self.dropout(l)
   324      1047    6471000.0   6180.5      0.1                  c = self.dropout(c)
   325      1047  100817000.0  96291.3      1.8                  g = torch.tanh(self.concat_r(torch.cat((ld, c), 1)))
   326      1047   99894000.0  95409.7      1.8                  t = torch.sigmoid(self.concat_rg(torch.cat((ld, c), 1)))
   327      1047    3227000.0   3082.1      0.1                  current_node_temp.append(g * t)
   328                                           
   329                                           
   330                                                   # batch_size x 1 x hidden_dim
   331      3144   29101000.0   9256.0      0.5          current_node = torch.stack(current_node_temp)
   332                                                   # this is the q of the current subtree
   333      3144   29904000.0   9511.5      0.5          current_embeddings = self.dropout(current_node)
   334                                           
   335                                                   # this gets the score calculation and relation between each 
   336                                                   # encoded token
   337                                                   # this is the a in equation (6)
   338      3144 1583401000.0 503626.3     28.5          current_attn = self.attn(current_embeddings.transpose(0, 1), encoder_outputs, seq_mask)
   339                                                   # the encoder_outputs are the h
   340                                           
   341                                                   # this is the context vector 
   342                                                   # equation (6)
   343      3144   48547000.0  15441.2      0.9          current_context = current_attn.bmm(encoder_outputs.transpose(0, 1))  # B x 1 x N
   344                                           
   345                                                   # the information to get the current quantity
   346      3144    2801000.0    890.9      0.1          batch_size = current_embeddings.size(0)
   347                                           
   348                                           
   349                                                   # predict the output (this node corresponding to output(number or operator)) with PADE
   350                                           
   351      3144   10256000.0   3262.1      0.2          repeat_dims = [1] * self.embedding_weight.dim()
   352      3144    1316000.0    418.6      0.0          repeat_dims[0] = batch_size
   353                                                   # self.embedding_weight:
   354                                                   #   1 x 2 x hidden_dim
   355                                                   #   this is the amount to weight the each hidden dim for 2 things
   356                                                   #       maybe the current context, and the leaf context? 
   357                                           
   358                                           
   359                                                   # repeat the embedding weight for each batch
   360                                                   # batch_size x 2 x hidden_dim
   361      3144   57214000.0  18197.8      1.0          embedding_weight1 = self.embedding_weight.repeat(*repeat_dims)  # B x input_size x N
   362                                           
   363                                                   # batch_size x (2 + number of numbers we have encodings for) x hidden_dim
   364                                                   # batch_size is the embeddings of the numbers
   365                                                   #   batch_size x nums_count x hidden_dim
   366      3144   64197000.0  20418.9      1.2          embedding_weight = torch.cat((embedding_weight1, xs, num_pades), dim=1)  # B x O x N
   367                                           
   368                                           
   369                                                   # get the embedding of a leaf
   370                                                   # embedding of a leaf is the concatenation of 
   371                                                   #   the node embedding and the embedding after attention
   372                                                   # this is the [q c]
   373                                                   # batch_size x 1 x 2*hidden_dim 
   374      3144   13063000.0   4154.9      0.2          leaf_input1 = torch.cat((current_node, current_context), 2)
   375                                                   # batch_size x 2*hidden_dim 
   376      3144    7004000.0   2227.7      0.1          leaf_input = leaf_input1.squeeze(1)
   377      3144   40189000.0  12782.8      0.7          leaf_input = self.dropout(leaf_input)
   378                                           
   379                                                   # p_leaf = nn.functional.softmax(self.is_leaf(leaf_input), 1)
   380                                                   # max pooling the embedding_weight
   381      3144  212378000.0  67550.3      3.8          embedding_weight_ = self.dropout(embedding_weight)
   382                                           
   383                                           
   384                                                   # get the scores between the leaves,  
   385                                                   # leaf_input is the embedding of a leaf
   386                                                   # TODO
   387                                                   # embedding_weight is the weight matrix of scaling the input hidden dims? (need to conform this is true)
   388                                                   # mask_nums is batch_size x max_nums_length and is 0 where the num comes from text
   389                                                   # equation (7) done here
   390                                                   # this is the log likliehood of generating token y from the specified vocab
   391                                                   #   doesnt seem like the full vocab is being used, only
   392      3144 2680647000.0 852623.1     48.2          num_score = self.score(leaf_input.unsqueeze(1), embedding_weight_, mask_nums, var_mask)
   393                                           
   394                                                   # get the predicted operation (classification)
   395                                                   # batch_size x num_ops 
   396      3144   66353000.0  21104.6      1.2          op = self.ops(leaf_input)
   397                                           
   398                                                   # return p_leaf, num_score, op, current_embeddings, current_attn
   399                                           
   400                                                   # this returns
   401                                                   #   num_score: batch_size x num_length
   402                                                   #       score values of each number
   403                                                   #   op: operation list
   404                                                   #       batch_size x num_ops
   405                                                   #   current_node: q : batch_size x 1 x hidden_size
   406                                                   #       goal vector for the subtree
   407                                                   #   current_context: c : batch_size x 1 x hidden_size
   408                                                   #       context vector for the subtree
   409                                                   #   embedding_weight : batch_size x num_length x hidden_size
   410                                                   #       number embeddings
   411      3144    1670000.0    531.2      0.0          return num_score, op, current_node, current_context, embedding_weight

Total time: 6.83341 s
File: /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py
Function: train_tree at line 254

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   254                                           @line_profiler.profile
   255                                           def train_tree(input_batch, input_length, target_batch, target_mask, target_length, target_equation_sonls, nums_stack_batch, num_size_batch, var_tokens_batch, solution_batch, generate_nums, encoder, num_x_predict, x_generate, x_to_q, predict, generate, merge, encoder_optimizer, num_x_predict_optimizer, x_generate_optimizer, x_to_q_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos, all_vars, english=False):
   256                                           
   257                                               # input_batch: padded inputs
   258                                               # input_length: length of the inputs (without padding)
   259                                               # target_batch: padded outputs
   260                                               # target_length: length of the outputs (without padding)
   261                                               # num_stack_batch: the corresponding nums lists
   262                                               # num_size_batch: number of numbers from the input text
   263                                               # generate_nums: numbers to generate
   264                                           
   265                                               # num_pos: positions of the numbers lists
   266                                           
   267                                           
   268                                               # # get max lengths of each equation and number of equations in the batch
   269                                               # max_equations_per_problem = 0
   270                                               # max_tokens_per_equation = 0
   271                                               # for batch in input_batch:
   272                                               #     for input_seq, input_length_temp, equations, equation_lengths, input_nums, input_nums_pos, num_stack in batch:
   273                                               #     # for equation in output_length:
   274                                               #         max_equations_per_problem = max(max_equations_per_problem, len(equations))
   275                                               #         for equation in equations:
   276                                               #             max_tokens_per_equation = max(max_tokens_per_equation, len(equation))
   277                                               
   278                                               # print("max equations per problem: ", max_equations_per_problem)
   279                                               # print('max tokens per equation: ', max_tokens_per_equation)
   280                                           
   281                                           
   282                                               # sequence mask for attention
   283                                               # 0s where in input, 1s where not in input
   284         5       4000.0    800.0      0.0      seq_mask = []
   285         5       7000.0   1400.0      0.0      max_len = max(input_length)
   286        55      12000.0    218.2      0.0      for i in input_length:
   287        50     209000.0   4180.0      0.0          seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])
   288         5    1012000.0 202400.0      0.0      seq_mask = torch.ByteTensor(seq_mask)
   289                                           
   290                                           
   291         5       9000.0   1800.0      0.0      unk = output_lang.word2index["UNK"]
   292                                           
   293                                               # Turn padded arrays into (equation x max_len x batch_size ) tensors, 
   294                                               # print("input batch", input_batch)
   295         5     912000.0 182400.0      0.0      input_var = torch.LongTensor(input_batch).transpose(0, 1)
   296                                               # print("input var", input_var)
   297                                               # print('target batch', target_batch)
   298         5    4461000.0 892200.0      0.1      target = torch.stack([torch.LongTensor(equation_set) for equation_set in target_batch], dim=-1)
   299                                               # print('target', target)
   300                                           
   301         5     810000.0 162000.0      0.0      padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)
   302         5       6000.0   1200.0      0.0      batch_size = len(input_length)
   303                                           
   304         5     258000.0  51600.0      0.0      encoder.train()
   305         5     716000.0 143200.0      0.0      predict.train()
   306         5     173000.0  34600.0      0.0      generate.train()
   307         5     101000.0  20200.0      0.0      merge.train()
   308         5     162000.0  32400.0      0.0      num_x_predict.train()
   309         5     151000.0  30200.0      0.0      x_generate.train()
   310         5     130000.0  26000.0      0.0      x_to_q.train()
   311                                           
   312         5          0.0      0.0      0.0      if USE_CUDA:
   313                                                   input_var = input_var.cuda()
   314                                                   seq_mask = seq_mask.cuda()
   315                                                   padding_hidden = padding_hidden.cuda()
   316                                                   # num_mask = num_mask.cuda()
   317                                           
   318                                               # Zero gradients of both optimizers
   319         5    4497000.0 899400.0      0.1      encoder_optimizer.zero_grad()
   320         5    1941000.0 388200.0      0.0      predict_optimizer.zero_grad()
   321         5    1754000.0 350800.0      0.0      generate_optimizer.zero_grad()
   322         5    1155000.0 231000.0      0.0      merge_optimizer.zero_grad()
   323         5    7678000.0    2e+06      0.1      num_x_predict_optimizer.zero_grad()
   324         5    2225000.0 445000.0      0.0      x_generate_optimizer.zero_grad()
   325         5    1213000.0 242600.0      0.0      x_to_q_optimizer.zero_grad()
   326                                           
   327                                           
   328                                               # Run words through encoder
   329                                               # embedding + dropout layer
   330                                               # encoder_outputs: num_batches x 512 q_0 vector
   331                                               # problem_output: max_length x num_batches x hidden_size
   332         5  327420000.0    7e+07      4.8      encoder_outputs, problem_output = encoder(input_var, input_length)
   333                                               
   334                                           
   335                                               # get the max length of the target equations and the number of equations in the batch
   336         5       4000.0    800.0      0.0      max_target_length = 0
   337         5       2000.0    400.0      0.0      max_num_equations = 0
   338        55      26000.0    472.7      0.0      for pair_equations_lengths in target_batch:
   339        50     139000.0   2780.0      0.0          max_target_length = max(max_target_length, *[len(equation) for equation in pair_equations_lengths])
   340        50      33000.0    660.0      0.0          max_num_equations = max(max_num_equations, len(pair_equations_lengths))
   341                                           
   342                                               # all_leafs = []
   343                                           
   344                                               # array of len of numbers that must be copied from the input text
   345         5      24000.0   4800.0      0.0      copy_num_len = [len(_) for _ in num_pos] 
   346                                               # max nums to copy
   347         5       6000.0   1200.0      0.0      num_size = max(copy_num_len)
   348                                           
   349                                               # for the numbers in the input text, get the embeddings
   350                                               # num_batches x num_size x hidden_size that correspond to the embeddings of the numbers
   351         5   12218000.0    2e+06      0.2      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size, encoder.hidden_size)
   352                                           
   353                                               # get the number of x's to generate
   354         5 1541257000.0    3e+08     22.6      num_x = num_x_predict(encoder_outputs)
   355                                               # gen_xs = []
   356                                           
   357                                               # the lang has X,Y,Z
   358                                           
   359                                               # need the order of variables in the vocab, so we place the embeddings in the correct order
   360         5       2000.0    400.0      0.0      target_ordering_of_vars = all_vars
   361                                               # temp_encoder: num_batches x max_length x hidden_size
   362         5      25000.0   5000.0      0.0      temp_encoder = encoder_outputs.transpose(0, 1)
   363                                           
   364         5       3000.0    600.0      0.0      batch_all_vars = []
   365                                           
   366         5      65000.0  13000.0      0.0      empty = torch.zeros(encoder.hidden_size).to(device)
   367        55      30000.0    545.5      0.0      for batch_num, batch_vars in enumerate(var_tokens_batch):
   368        50      10000.0    200.0      0.0          cur_bach_vars = []
   369        50   12384000.0 247680.0      0.2          xs = x_generate(len(batch_vars), temp_encoder[batch_num], all_nums_encoder_outputs[batch_num], problem_output[batch_num])
   370                                                   # get the generated variables
   371       150      60000.0    400.0      0.0          for variable in target_ordering_of_vars:
   372       100      18000.0    180.0      0.0              match = False
   373       272     234000.0    860.3      0.0              for var, x in zip(batch_vars, xs):
   374       172      31000.0    180.2      0.0                  if var == variable:
   375        86      35000.0    407.0      0.0                      cur_bach_vars.append(x)
   376        86      17000.0    197.7      0.0                      match = True
   377       100      15000.0    150.0      0.0              if not match:
   378        14       7000.0    500.0      0.0                  cur_bach_vars.append(empty)
   379        50     271000.0   5420.0      0.0          batch_all_vars.append(torch.stack(cur_bach_vars))
   380                                           
   381                                               # all_vars_embs: num_batches x num_vars x hidden_size
   382                                               # 0s if the var is not used in that observation
   383         5      35000.0   7000.0      0.0      all_vars_embs = torch.stack(batch_all_vars)
   384                                           
   385                                               
   386                                               # get the q vectors for each x
   387         5   12760000.0    3e+06      0.2      qs = x_to_q(temp_encoder, all_vars_embs)
   388                                           
   389                                               # number mask 
   390                                               # 0s where the numbers are from input, 1s where not in input
   391         5          0.0      0.0      0.0      num_mask = []
   392         5      13000.0   2600.0      0.0      max_num_size = max(num_size_batch) + len(generate_nums) 
   393        55      13000.0    236.4      0.0      for i in num_size_batch:
   394        50      12000.0    240.0      0.0          d = i + len(generate_nums) 
   395        50      63000.0   1260.0      0.0          num_mask.append([0] * d + [1] * (max_num_size - d))
   396         5     211000.0  42200.0      0.0      num_mask = torch.ByteTensor(num_mask)
   397                                               
   398                                               # for each batch, if there are variables in output lang [x,y,z] but group only has [x,y], mask would be [0,0,1]
   399         5          0.0      0.0      0.0      var_mask = []
   400                                               # first get variables in output lang 
   401                                               # var_list = [output_lang.variables for _ in target_batch]
   402                                               # for each equation group
   403        55      19000.0    345.5      0.0      for i, equ_group in enumerate(target_batch):
   404        50       8000.0    160.0      0.0          group_var_mask = []
   405        50     178000.0   3560.0      0.0          unique_in_equation = [output_lang.index2word[i] for i in list(set([el for arr in equ_group for el in arr]))]
   406       150      34000.0    226.7      0.0          for var in output_lang.variables:
   407                                                       # decoded = output_lang.index2word[var]
   408       100      18000.0    180.0      0.0              if var in unique_in_equation:
   409        86      15000.0    174.4      0.0                  group_var_mask.append(0)
   410                                                       else:
   411        14       4000.0    285.7      0.0                  group_var_mask.append(1)
   412        50       9000.0    180.0      0.0          var_mask.append(group_var_mask)
   413         5      21000.0   4200.0      0.0      var_mask = torch.ByteTensor(var_mask)
   414         5       9000.0   1800.0      0.0      addedQs = len(qs)
   415                                               # make sure to note that the updated encoder has the x vectors
   416                                           
   417                                           
   418                                               # index in the language where the special (operators) tokens end and input/output text begins
   419         5       2000.0    400.0      0.0      num_start = output_lang.num_start
   420                                               # 
   421         5       1000.0    200.0      0.0      all_outputs = []
   422         5       1000.0    200.0      0.0      final_tokens = []
   423        15       8000.0    533.3      0.0      for equation_count in range(max_num_equations):
   424        10      89000.0   8900.0      0.0          embeddings_stacks = [[] for _ in range(batch_size)]
   425        10      19000.0   1900.0      0.0          left_childs = [None for _ in range(batch_size)]
   426        10      34000.0   3400.0      0.0          current_equation_outputs = []
   427        10      51000.0   5100.0      0.0          current_equation = target[equation_count]
   428                                           
   429                                                   # make a TreeNode for each token
   430                                                   # node just has embedding and a left flag? 
   431        10     283000.0  28300.0      0.0          tempSplit = problem_output.split(1, dim=0)
   432                                                   # problem_output is q_0 for each token in equation, so use last one
   433        10     368000.0  36800.0      0.0          node_stacks = [[TreeNode(_.unsqueeze(0))] for _ in qs.transpose(0,1)[equation_count]]
   434                                                   
   435       120      61000.0    508.3      0.0          for t in range(max_target_length):
   436                                           
   437                                                       # predict gets the encodings and embeddings for the current node 
   438                                                       #   num_score: batch_size x num_length
   439                                                       #       likliehood prediction of each number
   440                                                       #   op: batch_size x num_ops
   441                                                       #       likliehood of the operator tokens
   442                                                       #   current_embeddings: batch_size x 1 x hidden_size
   443                                                       #       goal vector (q) for the current node 
   444                                                       #   current_context: batch_size x 1 x hidden_size
   445                                                       #       context vector (c) for the subtree
   446                                                       #   embedding_weight: batch_size x num_length x hidden_size
   447                                                       #       embeddings of the generate and copy numbers
   448                                           
   449       220  810444000.0    4e+06     11.9              num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(
   450       110      38000.0    345.5      0.0                  node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask, var_mask, all_vars_embs)
   451                                           
   452                                           
   453                                                       # this is mainly what we want to train
   454       110     714000.0   6490.9      0.0              outputs = torch.cat((op, num_score), 1)
   455       110      78000.0    709.1      0.0              current_equation_outputs.append(outputs)
   456                                           
   457                                                       # target[t] is the equation character at index t for each batch
   458                                                       #    target[t] = 1 x num_batches
   459                                                       # outputs is the strength of operators or a number token
   460                                                       # num_stack_batch is the cooresponding num lists
   461                                                       # num_start is where non-operators begin
   462                                                       # unk is unknown token
   463                                                       # returns
   464                                                       #   for position t in each equation
   465                                                       #       target_t: actual equation value
   466                                                       #       generate_input: equation value if its an operator
   467                                                       # target_t, generate_input = generate_tree_input(target[equation_count][t].tolist(), outputs, nums_stack_batch, num_start, unk)
   468       110    5987000.0  54427.3      0.1              target_t, generate_input = generate_tree_input(current_equation[t].tolist(), outputs, nums_stack_batch, num_start, unk)
   469                                                       # target[t] = target_t
   470       110      31000.0    281.8      0.0              if USE_CUDA:
   471                                                           generate_input = generate_input.cuda()
   472                                           
   473                                                       # takes:
   474                                                       #     generate a left and right child node with a label
   475                                                       #     current_embeddings: q : batch_size x 1 x hidden_dim
   476                                                       #     generate_input: [operator tokens at position t]
   477                                                       #     current_context: c : batch_size x 1 x hidden_dim
   478                                                       # returns
   479                                                       #     l_child: batch_size x hidden_dim
   480                                                       #          hidden state h_l:
   481                                                       #     r_child: batch_size x hidden_dim
   482                                                       #          hidden state h_r:
   483                                                       #     node_label_ : batch_size x embedding_size 
   484                                                       #          basically the context vector (c)
   485                                                       # the node generation takes the first half of equations (10) and (11) 
   486       110  130481000.0    1e+06      1.9              left_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)
   487       110      74000.0    672.7      0.0              left_childs = []
   488      1320   10087000.0   7641.7      0.1              for idx, l, r, node_stack, i, o in zip(range(batch_size), left_child.split(1), right_child.split(1),
   489       110      29000.0    263.6      0.0                                                  node_stacks, target_t, embeddings_stacks):
   490                                                           # current_token = output_lang.ids_to_tokens([i])
   491                                                           # current_equation = output_lang.ids_to_tokens(target.transpose(0,1)[idx])
   492                                                           #print("at token", current_token, "in", current_equation)
   493                                                           #print("current node_stack length", len(node_stack))
   494                                                           # for 
   495                                                           #   batch_num
   496                                                           #   the left child: h_l 
   497                                                           #   the right child: h_r
   498      1100     376000.0    341.8      0.0                  if len(node_stack) != 0:
   499       478     610000.0   1276.2      0.0                      node = node_stack.pop()
   500                                                               #print("removed last from node_stack, now", len(node_stack), "elems")
   501                                                           else:
   502       622     163000.0    262.1      0.0                      left_childs.append(None)
   503       622      69000.0    110.9      0.0                      continue
   504                                           
   505                                                           # i is the num in language of where that specific language token is
   506                                                           # if i is an operator
   507       478    3388000.0   7087.9      0.0                  if i < num_start:
   508                                                               #print(current_token, "is an operator, making a left and right node")
   509                                                               # make a left and right tree node
   510       271     286000.0   1055.4      0.0                      node_stack.append(TreeNode(r))
   511       271     260000.0    959.4      0.0                      node_stack.append(TreeNode(l, left_flag=True))
   512                                                               # save the embedding of the operator 
   513                                                               # terminal means a leaf node
   514       271    1480000.0   5461.3      0.0                      o.append(TreeEmbedding(node_label[idx].unsqueeze(0), False))
   515                                                               #print("saving node embedding to o (non terminal node), and r, and l to node_stack. o now of size", len(o), "node_stack of size", len(node_stack))
   516                                                           else:
   517                                                               # otherwise its either a number from the input equation or a copy number
   518                                                               # we have a list (o) of the current nodes in the tree
   519                                                               # if we have a leaf node at the top of the stack, get it.
   520                                                               # next element in the stack must be an operator, so get it 
   521                                                               # and combine the new node, operator, and other element
   522                                           
   523                                                               # current_nums_embedding: batch_size x num_length x hidden_size
   524                                                               # current_num = num_embedding of the number selected
   525       207    2461000.0  11888.9      0.0                      current_num = current_nums_embeddings[idx, i - num_start].unsqueeze(0)
   526                                                               # while there are tokens in the embedding stack and the last element IS a leaf node
   527       328     227000.0    692.1      0.0                      while len(o) > 0 and o[-1].terminal:
   528                                                                   #print("terminal element in o, getting terminal element and operator, and merging")
   529                                                                   # get the two elements from it
   530       121     940000.0   7768.6      0.0                          sub_stree = o.pop()
   531       121      99000.0    818.2      0.0                          op = o.pop()
   532                                                                   # contains equation (13)
   533                                                                   # this combines a left and right tree along with a node
   534       121   21568000.0 178247.9      0.3                          current_num = merge(op.embedding, sub_stree.embedding, current_num)
   535                                                                   #print('merged. o now of size', len(o))
   536                                                               # then re-add the node back to the stack
   537                                                               #print("adding current_num to o (terminal node)")
   538       207     275000.0   1328.5      0.0                      o.append(TreeEmbedding(current_num, True))
   539       478     196000.0    410.0      0.0                  if len(o) > 0 and o[-1].terminal:
   540                                                               #print("terminal element in o, adding to left child")
   541                                                               # left_childs is a running vector of the sub tree embeddings "t" 
   542                                                               # need this for generation of the right q
   543       207      85000.0    410.6      0.0                      left_childs.append(o[-1].embedding)
   544                                                           else:
   545       271      81000.0    298.9      0.0                      left_childs.append(None)
   546                                                   # tree has been built. now predict the = " " token
   547        20   70799000.0    4e+06      1.0          num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(
   548        10       3000.0    300.0      0.0              node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask, var_mask, all_vars_embs)
   549        10      56000.0   5600.0      0.0          final_token = torch.cat((op, num_score), 1)
   550        10       9000.0    900.0      0.0          final_tokens.append(final_token)
   551        10     170000.0  17000.0      0.0          all_outputs.append(torch.stack(current_equation_outputs, dim = 1))
   552                                           
   553                                               # final tokens:
   554         5      23000.0   4600.0      0.0      preds_final_tokens_all = torch.stack(final_tokens, dim = 1)
   555         5      19000.0   3800.0      0.0      preds_final_tokens_all_flattened = preds_final_tokens_all.view(-1, preds_final_tokens_all.size(-1))
   556                                           
   557                                               # batch solutons: 
   558                                               # mask for solutions
   559         5       2000.0    400.0      0.0      solution_batch_mask = []
   560         5          0.0      0.0      0.0      final_solutions = []
   561        55      28000.0    509.1      0.0      for i, batch in enumerate(solution_batch):
   562        50      13000.0    260.0      0.0          if len(batch) < max_num_equations:
   563        14      18000.0   1285.7      0.0              temp_batch = batch + [0 for _ in range(max_num_equations - len(batch))]
   564        14          0.0      0.0      0.0              final_solutions.append(temp_batch)
   565        14      21000.0   1500.0      0.0              solution_batch_mask.append([1 for _ in range(len(batch))] + [0 for _ in range(max_num_equations - len(batch))])
   566                                                   else:
   567        36       5000.0    138.9      0.0              final_solutions.append(batch)
   568        36      43000.0   1194.4      0.0              solution_batch_mask.append([1 for _ in range(len(batch))])
   569                                           
   570         5     111000.0  22200.0      0.0      print('equation output')
   571         5       2000.0    400.0      0.0      same = 0
   572         5          0.0      0.0      0.0      lengths = 0 
   573        55     110000.0   2000.0      0.0      for i, batch in enumerate(preds_final_tokens_all):
   574       150     239000.0   1593.3      0.0          for j, prediction in enumerate(batch):
   575       100      27000.0    270.0      0.0              if final_solutions[i][j] != 0:
   576        86      12000.0    139.5      0.0                  lengths += 1
   577        86    2800000.0  32558.1      0.0                  print(f"    preds for batch {i}, equation {j}: {output_lang.index2word[torch.argmax(prediction).item()]}, actual: {output_lang.index2word[final_solutions[i][j]]}")
   578        86     237000.0   2755.8      0.0                  if output_lang.index2word[torch.argmax(prediction).item()] == output_lang.index2word[final_solutions[i][j]]:
   579                                                               same += 1
   580                                           
   581                                           
   582                                               # loss the equation result
   583         5      67000.0  13400.0      0.0      solution_batch_mask = torch.ByteTensor(solution_batch_mask)
   584         5      30000.0   6000.0      0.0      final_solutions_flattened = torch.LongTensor(final_solutions).view(-1)
   585                                           
   586                                               # solutions_all_loss
   587         5    4356000.0 871200.0      0.1      solutions_raw_loss = torch.nn.CrossEntropyLoss(reduction='none')(preds_final_tokens_all_flattened, final_solutions_flattened.to(device) )
   588                                               
   589         5      19000.0   3800.0      0.0      solutions_loss = solutions_raw_loss.view(preds_final_tokens_all.size(0), preds_final_tokens_all.size(1))
   590                                               # apply mask to loss
   591         5      54000.0  10800.0      0.0      solutions_loss_final = solutions_loss * solution_batch_mask.float().to(device)
   592         5     982000.0 196400.0      0.0      solutions_loss_final = solutions_loss_final.sum()
   593         5   12445000.0    2e+06      0.2      print('solution loss', solutions_loss_final)
   594                                           
   595                                               # all_node_outputs:  
   596                                               #  for each equation 
   597                                               #    for each token in equation: 
   598                                               #      the current scoring of nums for each batch
   599                                               # 
   600                                               # transform to 
   601                                               # all_node_outputs2: for each batch:
   602                                               #   the current scoring of nums for each token in equation
   603                                               # = batch_size x max_len x num_nums
   604         5     199000.0  39800.0      0.0      all_outputs_stacked = torch.stack(all_outputs, dim=-1)
   605                                           
   606                                               # target: num_equations x num_tokens x batch_size
   607         5      57000.0  11400.0      0.0      target2 = target.transpose(0, 1).contiguous()
   608         5      19000.0   3800.0      0.0      target3 = target2.transpose(1, 2).contiguous()
   609                                               # actual
   610                                               # target4: batch_size x max_len x num_equations
   611         5      18000.0   3600.0      0.0      target4 = target3.transpose(0,1).contiguous()
   612                                           
   613         5       2000.0    400.0      0.0      if USE_CUDA:
   614                                                   # all_leafs = all_leafs.cuda()
   615                                                   all_node_outputs2 = all_outputs
   616                                                   target = target.cuda()
   617                                           
   618                                           
   619         5       1000.0    200.0      0.0      target_length_filled = []
   620        55       7000.0    127.3      0.0      for problem in target_length:
   621        50      14000.0    280.0      0.0          current_num_equations = len(problem)
   622        50       6000.0    120.0      0.0          if current_num_equations < max_num_equations:
   623        14      13000.0    928.6      0.0              problem += [0 for _ in range(max_num_equations - current_num_equations)]
   624        50      16000.0    320.0      0.0          target_length_filled.append(problem)
   625                                               # target_length_filled = torch.Tensor(target_length_filled)
   626                                               # loss = masked_cross_entropy(all_node_outputs2, target, target_length)
   627                                               # loss the number of equations
   628         5      45000.0   9000.0      0.0      actual_num_x = torch.LongTensor([(len(i) - 1) if i != [] else 0 for i in var_tokens_batch])
   629         5     235000.0  47000.0      0.0      num_x_loss = torch.nn.CrossEntropyLoss()(num_x, actual_num_x.to(device) )
   630                                               # print('num x loss', num_x_loss)
   631                                               # print('number of equations/variables')
   632                                               # for i, batch in enumerate(num_x):
   633                                               #     print(f"    preds for batch {i}: {batch} equations. Actual: {actual_num_x[i]}")
   634                                               # print('     mse loss', loss)
   635                                           
   636         5     493000.0  98600.0      0.0      target_mask1 = 1 - torch.ByteTensor(target_mask)
   637         5      11000.0   2200.0      0.0      target_mask_tensor = target_mask1.transpose(1,2)
   638                                               # for batch in target_mask:   
   639                                               #     batch_equs = []
   640                                               #     for equation in batch:
   641                                               #         batch_equs.append(torch.Tensor(equation))
   642                                               #     batch_stack = torch.stack(batch_equs, dim=0)
   643                                           
   644                                               # target_mask_stacked = torch.stack([torch.Tensor(i) for i in target_mask], dim=-1)
   645                                           
   646         5      30000.0   6000.0      0.0      print('token lists of')
   647         5      29000.0   5800.0      0.0      if solutions_loss_final > 100000:
   648                                                   print('solutions high')
   649                                                   raise Exception('high') 
   650         5      17000.0   3400.0      0.0      if  num_x_loss > 100000:
   651                                                   raise Exception('num x high') 
   652         5      30000.0   6000.0      0.0      loss = solutions_loss_final + num_x_loss
   653                                               # equation_loss = solutions_loss_final
   654        15      26000.0   1733.3      0.0      for i in range(max_num_equations):
   655                                                   # target4 = batch_size x max_len x num_equations
   656                                                   # equation_target = batch_size x max_len 
   657        10      47000.0   4700.0      0.0          equation_target = target4[..., i]
   658        10      17000.0   1700.0      0.0          equation_mask = target_mask_tensor[..., i]
   659        10      20000.0   2000.0      0.0          target_flattened = equation_target.view(-1)
   660                                                   # equation_mask_flattened = equation_mask.view(-1)
   661                                                   # for each batch 
   662        10       1000.0    100.0      0.0          target_tokens = []
   663       110     127000.0   1154.5      0.0          for actuals in equation_target:
   664                                                       # print("target", actuals)
   665       100    1057000.0  10570.0      0.0              target_tokens.append([output_lang.index2word[_] for _ in actuals])
   666                                                   # print("target combined", target_tokens)
   667                                                   # all_outputs_stacked = batch_size x max_len x vocab_len x num_equations 
   668                                                   # predictions = batch_size x max_len x vocab_len
   669        10      26000.0   2600.0      0.0          predictions = all_outputs_stacked[..., i]
   670        10     668000.0  66800.0      0.0          pred_distribution = predictions.log_softmax(-1)
   671       110     176000.0   1600.0      0.0          for j, batch in enumerate(predictions):
   672       100      14000.0    140.0      0.0              eqn_preds = []
   673      1200    1175000.0    979.2      0.0              for token in batch:
   674      1100    2692000.0   2447.3      0.0                  eqn_preds.append(output_lang.index2word[torch.argmax(token).item()])
   675       100     339000.0   3390.0      0.0              print(f"    batch {j}, equation {i}" )
   676       100     209000.0   2090.0      0.0              print(f"        prkediction: {eqn_preds[0:target_length[j][i]]}")
   677       100     163000.0   1630.0      0.0              print(f"        actual: {target_tokens[j][0:target_length[j][i]]}")
   678       428     125000.0    292.1      0.0              for k in range(len(eqn_preds[0:target_length[j][i]])):
   679       328     157000.0    478.7      0.0                  if eqn_preds[0:target_length[j][i]][k] == target_tokens[j][0:target_length[j][i]][k]:
   680        59       8000.0    135.6      0.0                      same += 1
   681       328      61000.0    186.0      0.0                  lengths += 1
   682                                                       # print(f'{same} so far')
   683                                                   # preds_flattened = predictions.view(-1, predictions.size(-1))
   684        10      31000.0   3100.0      0.0          preds_flattened = pred_distribution.view(-1, predictions.size(-1))
   685                                           
   686        10    1179000.0 117900.0      0.0          tempLoss = torch.nn.CrossEntropyLoss(reduction='none')(preds_flattened, target_flattened.to(device))
   687                                                   # print('tempLoss', tempLoss)
   688                                                   # reshape loss:
   689        10      71000.0   7100.0      0.0          tempLoss2 = tempLoss.view(equation_target.size(0), equation_target.size(1))
   690                                                   # apply mask to loss
   691        10     356000.0  35600.0      0.0          tempLoss3 = tempLoss2 * equation_mask.float().to(device)
   692                                           
   693                                                   # equation_loss += tempLoss3.sum() 
   694        10     199000.0  19900.0      0.0          print(f'equ {i} loss, {tempLoss3.sum()}')
   695                                                   # for lossT, token in zip(tempLoss3, target_flattened):
   696                                                   #     for i, l in enumerate(lossT):
   697                                                   #         print(f"batch: {i}")
   698                                                   #         print(f"    token {output_lang.index2word[token]} loss: {l}")
   699                                                   #         if l > 900000:
   700                                                   #             print("ANOMOLY")
   701                                                   #             print('prediction list', predictions[i])
   702        10       3000.0    300.0      0.0          if loss is None:
   703                                                       loss = tempLoss3.sum()
   704                                                   else:
   705        10     102000.0  10200.0      0.0              loss += tempLoss3.sum()
   706                                           
   707                                               # full loss = equation loss + number of equations loss + solutions loss
   708                                               # loss = equation_loss + num_x_loss
   709         5     677000.0 135400.0      0.0      print('total loss', loss)
   710         5 3800897000.0    8e+08     55.6      loss.backward()
   711                                           
   712                                               # make_dot(loss).render("loss")
   713                                           
   714                                           
   715         5      47000.0   9400.0      0.0      return loss.item(), same/lengths

Total time: 16.852 s
File: /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py
Function: evaluate_tree at line 717

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   717                                           @line_profiler.profile
   718                                           def evaluate_tree(input_batch, input_length, generate_nums, encoder, predict, generate, x_generate, x_to_q, num_x_predict, merge, output_lang, num_pos, actual_num_x, beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):
   719                                           
   720                                               # seq_mask = torch.ByteTensor(1, input_length).fill_(0)
   721       150     101000.0    673.3      0.0      seq_mask = []
   722       150     102000.0    680.0      0.0      max_len = input_length
   723                                               # for i in input_length:
   724       150     561000.0   3740.0      0.0      seq_mask = [0 for _ in range(max_len)]
   725       150    2539000.0  16926.7      0.0      seq_mask = torch.ByteTensor(seq_mask)
   726                                           
   727                                           
   728       150    1397000.0   9313.3      0.0      input_var = torch.LongTensor(input_batch).transpose(-1, 0)
   729                                               # target = torch.stack([torch.LongTensor(equation_set) for equation_set in target_batch], dim=-1)
   730                                           
   731       150    9823000.0  65486.7      0.1      padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)
   732       150      50000.0    333.3      0.0      batch_size = 1
   733                                           
   734                                               # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)
   735                                               # input_var = torch.LongTensor(input_batch).unsqueeze(1)
   736                                           
   737                                               # num_mask = torch.ByteTensor(1, len(num_pos) + len(generate_nums)).fill_(0)
   738                                           
   739                                               # Set to not-training mode to disable dropout
   740       150    6963000.0  46420.0      0.0      encoder.eval()
   741       150    8931000.0  59540.0      0.1      predict.eval()
   742       150    4426000.0  29506.7      0.0      generate.eval()
   743       150    2609000.0  17393.3      0.0      merge.eval()
   744       150    3702000.0  24680.0      0.0      num_x_predict.eval()
   745       150    3759000.0  25060.0      0.0      x_generate.eval()
   746       150    3172000.0  21146.7      0.0      x_to_q.eval()
   747                                               
   748                                           
   749                                               # padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)
   750                                           
   751                                               # batch_size = 1
   752                                           
   753       150      81000.0    540.0      0.0      if USE_CUDA:
   754                                                   input_var = input_var.cuda()
   755                                                   seq_mask = seq_mask.cuda()
   756                                                   padding_hidden = padding_hidden.cuda()
   757                                                   # num_mask = num_mask.cuda()
   758                                               # Run words through encoder
   759                                           
   760                                               # encoder_outputs, problem_output = encoder(input_var, [input_length])
   761                                               # TODO: make sure input var moves correctly
   762       150 1172542000.0    8e+06      7.0      encoder_outputs, problem_output = encoder(input_var.unsqueeze(0).transpose(0,1), [input_length])
   763                                           
   764       150      38000.0    253.3      0.0      max_target_length = 0
   765       150      31000.0    206.7      0.0      max_num_equations = 3
   766                                           
   767       150     248000.0   1653.3      0.0      copy_num_len = [len(num_pos)] 
   768       150      41000.0    273.3      0.0      num_size = len(num_pos)
   769                                               # num_size = 1
   770       300   34182000.0 113940.0      0.2      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,
   771       150      35000.0    233.3      0.0                                                                encoder.hidden_size)
   772                                               # all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,
   773                                               #                                                           encoder.hidden_size)
   774       150 5718524000.0    4e+07     33.9      num_x = num_x_predict(encoder_outputs, eval = True)
   775       150    2493000.0  16620.0      0.0      num_to_gen = num_x.argmax() + 1
   776                                           
   777       150     535000.0   3566.7      0.0      temp_encoder = encoder_outputs.transpose(0, 1)
   778       150      95000.0    633.3      0.0      num_start = output_lang.num_start
   779                                               # B x P x N
   780                                           
   781                                           
   782       150      39000.0    260.0      0.0      batch_all_vars = []
   783       150    1514000.0  10093.3      0.0      empty = torch.zeros(encoder.hidden_size)
   784       150      24000.0    160.0      0.0      cur_bach_vars = []
   785       150   47395000.0 315966.7      0.3      xs = x_generate(num_to_gen, temp_encoder[0], all_nums_encoder_outputs[0], problem_output[0])
   786                                           
   787                                               # get the generated variables
   788                                               # for variable in target_ordering_of_vars:
   789                                               #     match = False
   790                                               #     for var, x in zip(batch_vars, xs):
   791                                               #         if var == variable:
   792                                               #             cur_bach_vars.append(x)
   793                                               #             match = True
   794                                               #     if not match:
   795                                               #         cur_bach_vars.append(empty)
   796                                               # batch_all_vars.append(torch.stack(cur_bach_vars))
   797       150    1403000.0   9353.3      0.0      all_vars_embs = torch.stack(xs).unsqueeze(0)
   798                                           
   799       150   64527000.0 430180.0      0.4      qs = x_to_q(temp_encoder, all_vars_embs)
   800                                               # Prepare input and output variables
   801                                           
   802                                           
   803                                               # empty = torch.zeros(encoder.hidden_size)
   804                                           
   805                                               # evaulation uses beam search
   806                                               # key is how the beams are compared
   807       150      42000.0    280.0      0.0      output = []
   808       150      14000.0     93.3      0.0      output_tokens = []
   809                                           
   810                                               # equations to do
   811       450  326034000.0 724520.0      1.9      for num_x in range(num_to_gen):
   812                                               # for num_x in range(actual_num_x):
   813       300    1305000.0   4350.0      0.0          embeddings_stacks = [[] for _ in range(batch_size)]
   814       300     305000.0   1016.7      0.0          left_childs = [None for _ in range(batch_size)]
   815       300    9112000.0  30373.3      0.1          node_stacks = [[TreeNode(_.unsqueeze(0))] for _ in qs.transpose(0,1)[num_x]]
   816                                                   # beans for the equation
   817       300    5664000.0  18880.0      0.0          beams = [TreeBeam(0.0, node_stacks, embeddings_stacks, left_childs, [])]
   818      1020     561000.0    550.0      0.0          for t in range(max_length):
   819       900  100312000.0 111457.8      0.6              current_beams = []
   820      4200    2683000.0    638.8      0.0              while len(beams) > 0:
   821      3300    7424000.0   2249.7      0.0                  b = beams.pop()
   822      3300    1869000.0    566.4      0.0                  if len(b.node_stack[0]) == 0:
   823       576     143000.0    248.3      0.0                      current_beams.append(b)
   824       576      72000.0    125.0      0.0                      continue
   825                                                           # left_childs = torch.stack(b.left_childs)
   826      2724     615000.0    225.8      0.0                  left_childs = b.left_childs
   827                                           
   828      2724 4314923000.0    2e+06     25.6                  num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(b.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, None, None, all_vars_embs)
   829                                           
   830                                                           # leaf = p_leaf[:, 0].unsqueeze(1)
   831                                                           # repeat_dims = [1] * leaf.dim()
   832                                                           # repeat_dims[1] = op.size(1)
   833                                                           # leaf = leaf.repeat(*repeat_dims)
   834                                                           #
   835                                                           # non_leaf = p_leaf[:, 1].unsqueeze(1)
   836                                                           # repeat_dims = [1] * non_leaf.dim()
   837                                                           # repeat_dims[1] = num_score.size(1)
   838                                                           # non_leaf = non_leaf.repeat(*repeat_dims)
   839                                                           #
   840                                                           # p_leaf = torch.cat((leaf, non_leaf), dim=1)
   841      2724   53311000.0  19570.9      0.3                  out_score = nn.functional.log_softmax(torch.cat((op, num_score), dim=1), dim=1)
   842                                           
   843                                                           # out_score = p_leaf * out_score
   844                                           
   845                                                           # topv:
   846                                                           #   largest elements in the out_score
   847                                                           # topi:
   848                                                           #   indexes of the largest elements 
   849      2724   32120000.0  11791.5      0.2                  topv, topi = out_score.topk(beam_size)
   850                                           
   851                                                           # is_leaf = int(topi[0])
   852                                                           # if is_leaf:
   853                                                           #     topv, topi = op.topk(1)
   854                                                           #     out_token = int(topi[0])
   855                                                           # else:
   856                                                           #     topv, topi = num_score.topk(1)
   857                                                           #     out_token = int(topi[0]) + num_start
   858                                           
   859                                                           # for the largest element, and its index
   860     16344  196893000.0  12046.8      1.2                  for tv, ti in zip(topv.split(1, dim=1), topi.split(1, dim=1)):
   861     13620   39487000.0   2899.2      0.2                      current_node_stack = copy_list(b.node_stack)
   862     13620    3528000.0    259.0      0.0                      current_left_childs = []
   863     13620   27310000.0   2005.1      0.2                      current_embeddings_stacks = copy_list(b.embedding_stack)
   864     13620   79262000.0   5819.5      0.5                      current_out = copy.deepcopy(b.out)
   865                                           
   866                                                               # the predicted token is that of the highest score relation
   867     13620   24088000.0   1768.6      0.1                      out_token = int(ti)
   868                                                               # save token 
   869     13620    4050000.0    297.4      0.0                      current_out.append(out_token)
   870                                           
   871     13620   12761000.0    936.9      0.1                      node = current_node_stack[0].pop()
   872                                           
   873                                                               # if the predicted token is an operator
   874     13620    3459000.0    254.0      0.0                      if out_token < num_start:
   875                                                                   # this is the token to generate l and r from
   876      5904   56412000.0   9554.9      0.3                          generate_input = torch.LongTensor([out_token])
   877      5904    2019000.0    342.0      0.0                          if USE_CUDA:
   878                                                                       generate_input = generate_input.cuda()
   879                                                                   # get the left and right children and current label
   880      5904 3048940000.0 516419.4     18.1                          left_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)
   881                                           
   882      5904   17805000.0   3015.8      0.1                          current_node_stack[0].append(TreeNode(right_child))
   883      5904    9247000.0   1566.2      0.1                          current_node_stack[0].append(TreeNode(left_child, left_flag=True))
   884                                           
   885      5904   69784000.0  11819.8      0.4                          current_embeddings_stacks[0].append(TreeEmbedding(node_label[0].unsqueeze(0), False))
   886                                                               else:
   887                                                                   # predicted token is a number
   888                                                                   # get the token embedding - embedding of either the generate num or copy num
   889      7716   56179000.0   7280.8      0.3                          current_num = current_nums_embeddings[0, out_token - num_start].unsqueeze(0)
   890                                                                   
   891                                                                   # if we are a right node (there is a left node and operator)
   892     11016    6936000.0    629.6      0.0                          while len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:
   893      3300    1709000.0    517.9      0.0                              sub_stree = current_embeddings_stacks[0].pop()
   894      3300    1135000.0    343.9      0.0                              op = current_embeddings_stacks[0].pop()
   895      3300  444817000.0 134793.0      2.6                              current_num = merge(op.embedding, sub_stree.embedding, current_num)
   896                                                                   # save node (or subtree) to the embeddings list
   897      7716    8741000.0   1132.8      0.1                          current_embeddings_stacks[0].append(TreeEmbedding(current_num, True))
   898     13620   12507000.0    918.3      0.1                      if len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:
   899      7716    2947000.0    381.9      0.0                          current_left_childs.append(current_embeddings_stacks[0][-1].embedding)
   900                                                               else:
   901      5904    2743000.0    464.6      0.0                          current_left_childs.append(None)
   902                                                               # the beam "score" is the sum of the associations 
   903     13620  240159000.0  17632.8      1.4                      current_beams.append(TreeBeam(b.score+float(tv), current_node_stack, current_embeddings_stacks,current_left_childs, current_out))
   904                                                       # order beam by highest to lowest
   905       900    7213000.0   8014.4      0.0              beams = sorted(current_beams, key=lambda x: x.score, reverse=True)
   906       900     924000.0   1026.7      0.0              beams = beams[:beam_size]
   907       900     152000.0    168.9      0.0              flag = True
   908      5400    1650000.0    305.6      0.0              for b in beams:
   909      4500    1802000.0    400.4      0.0                  if len(b.node_stack[0]) != 0:
   910      3004     395000.0    131.5      0.0                      flag = False
   911       900     295000.0    327.8      0.0              if flag:
   912       180      60000.0    333.3      0.0                  break
   913       300    3815000.0  12716.7      0.0          top_beam = beams[0]
   914                                                   # predict equation = 
   915       300  486489000.0    2e+06      2.9          num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(top_beam.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, None, None, all_vars_embs)
   916       300   22245000.0  74150.0      0.1          token_options = torch.cat((op, num_score), dim=1)
   917       300    5072000.0  16906.7      0.0          pred_token = output_lang.index2word[int(torch.argmax(token_options).item())]
   918                                           
   919       300     395000.0   1316.7      0.0          output.append(beams[0].out)
   920       300     162000.0    540.0      0.0          output_tokens.append(pred_token)
   921                                           
   922                                                   # num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(b.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, None, None, all_vars_embs)
   923       150      49000.0    326.7      0.0      return output, output_tokens

  5.56 seconds - /Users/home/school/thesis/math_seq2tree/src/models.py:280 - forward
  6.83 seconds - /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py:254 - train_tree
 16.85 seconds - /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py:717 - evaluate_tree
