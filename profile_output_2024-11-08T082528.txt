Timer unit: 1e-09 s

Total time: 3.34656 s
File: /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py
Function: train_tree at line 254

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   254                                           @line_profiler.profile
   255                                           def train_tree(input_batch, input_length, target_batch, target_mask, target_length, target_equation_sonls, nums_stack_batch, num_size_batch, var_tokens_batch, solution_batch, generate_nums, encoder, num_x_predict, x_generate, x_to_q, predict, generate, merge, encoder_optimizer, num_x_predict_optimizer, x_generate_optimizer, x_to_q_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos, all_vars, english=False):
   256                                           
   257                                               # input_batch: padded inputs
   258                                               # input_length: length of the inputs (without padding)
   259                                               # target_batch: padded outputs
   260                                               # target_length: length of the outputs (without padding)
   261                                               # num_stack_batch: the corresponding nums lists
   262                                               # num_size_batch: number of numbers from the input text
   263                                               # generate_nums: numbers to generate
   264                                           
   265                                               # num_pos: positions of the numbers lists
   266                                           
   267                                           
   268                                               # # get max lengths of each equation and number of equations in the batch
   269                                               # max_equations_per_problem = 0
   270                                               # max_tokens_per_equation = 0
   271                                               # for batch in input_batch:
   272                                               #     for input_seq, input_length_temp, equations, equation_lengths, input_nums, input_nums_pos, num_stack in batch:
   273                                               #     # for equation in output_length:
   274                                               #         max_equations_per_problem = max(max_equations_per_problem, len(equations))
   275                                               #         for equation in equations:
   276                                               #             max_tokens_per_equation = max(max_tokens_per_equation, len(equation))
   277                                               
   278                                               # print("max equations per problem: ", max_equations_per_problem)
   279                                               # print('max tokens per equation: ', max_tokens_per_equation)
   280                                           
   281                                           
   282                                               # sequence mask for attention
   283                                               # 0s where in input, 1s where not in input
   284         1       1000.0   1000.0      0.0      seq_mask = []
   285         1       2000.0   2000.0      0.0      max_len = max(input_length)
   286        65       9000.0    138.5      0.0      for i in input_length:
   287        64     190000.0   2968.8      0.0          seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])
   288         1    1246000.0    1e+06      0.0      seq_mask = torch.ByteTensor(seq_mask)
   289                                           
   290                                           
   291         1          0.0      0.0      0.0      unk = output_lang.word2index["UNK"]
   292                                           
   293                                               # Turn padded arrays into (equation x max_len x batch_size ) tensors, 
   294                                               # print("input batch", input_batch)
   295         1     838000.0 838000.0      0.0      input_var = torch.LongTensor(input_batch).transpose(0, 1)
   296                                               # print("input var", input_var)
   297                                               # print('target batch', target_batch)
   298         1    3427000.0    3e+06      0.1      target = torch.stack([torch.LongTensor(equation_set) for equation_set in target_batch], dim=-1)
   299                                               # print('target', target)
   300                                           
   301         1     335000.0 335000.0      0.0      padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)
   302         1          0.0      0.0      0.0      batch_size = len(input_length)
   303                                           
   304         1      47000.0  47000.0      0.0      encoder.train()
   305         1      47000.0  47000.0      0.0      predict.train()
   306         1      23000.0  23000.0      0.0      generate.train()
   307         1      13000.0  13000.0      0.0      merge.train()
   308         1      19000.0  19000.0      0.0      num_x_predict.train()
   309         1      19000.0  19000.0      0.0      x_generate.train()
   310         1      17000.0  17000.0      0.0      x_to_q.train()
   311                                           
   312         1          0.0      0.0      0.0      if USE_CUDA:
   313                                                   input_var = input_var.cuda()
   314                                                   seq_mask = seq_mask.cuda()
   315                                                   padding_hidden = padding_hidden.cuda()
   316                                                   # num_mask = num_mask.cuda()
   317                                           
   318                                               # Zero gradients of both optimizers
   319         1    1333000.0    1e+06      0.0      encoder_optimizer.zero_grad()
   320         1      67000.0  67000.0      0.0      predict_optimizer.zero_grad()
   321         1      48000.0  48000.0      0.0      generate_optimizer.zero_grad()
   322         1      43000.0  43000.0      0.0      merge_optimizer.zero_grad()
   323         1      46000.0  46000.0      0.0      num_x_predict_optimizer.zero_grad()
   324         1      43000.0  43000.0      0.0      x_generate_optimizer.zero_grad()
   325         1      40000.0  40000.0      0.0      x_to_q_optimizer.zero_grad()
   326                                           
   327                                           
   328                                               # Run words through encoder
   329                                               # embedding + dropout layer
   330                                               # encoder_outputs: num_batches x 512 q_0 vector
   331                                               # problem_output: max_length x num_batches x hidden_size
   332         1  159477000.0    2e+08      4.8      encoder_outputs, problem_output = encoder(input_var, input_length)
   333                                               
   334                                           
   335                                               # get the max length of the target equations and the number of equations in the batch
   336         1       1000.0   1000.0      0.0      max_target_length = 0
   337         1          0.0      0.0      0.0      max_num_equations = 0
   338        65      13000.0    200.0      0.0      for pair_equations_lengths in target_batch:
   339        64      59000.0    921.9      0.0          max_target_length = max(max_target_length, *[len(equation) for equation in pair_equations_lengths])
   340        64      14000.0    218.8      0.0          max_num_equations = max(max_num_equations, len(pair_equations_lengths))
   341                                           
   342                                               # all_leafs = []
   343                                           
   344                                               # array of len of numbers that must be copied from the input text
   345         1      10000.0  10000.0      0.0      copy_num_len = [len(_) for _ in num_pos] 
   346                                               # max nums to copy
   347         1       1000.0   1000.0      0.0      num_size = max(copy_num_len)
   348                                           
   349                                               # for the numbers in the input text, get the embeddings
   350                                               # num_batches x num_size x hidden_size that correspond to the embeddings of the numbers
   351         1    9472000.0    9e+06      0.3      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size, encoder.hidden_size)
   352                                           
   353                                               # get the number of x's to generate
   354         1  490052000.0    5e+08     14.6      num_x = num_x_predict(encoder_outputs)
   355                                               # gen_xs = []
   356                                           
   357                                               # the lang has X,Y,Z
   358                                           
   359                                               # need the order of variables in the vocab, so we place the embeddings in the correct order
   360         1          0.0      0.0      0.0      target_ordering_of_vars = all_vars
   361                                               # temp_encoder: num_batches x max_length x hidden_size
   362         1       5000.0   5000.0      0.0      temp_encoder = encoder_outputs.transpose(0, 1)
   363                                           
   364         1          0.0      0.0      0.0      batch_all_vars = []
   365                                           
   366         1      10000.0  10000.0      0.0      empty = torch.zeros(encoder.hidden_size).to(device)
   367        65      37000.0    569.2      0.0      for batch_num, batch_vars in enumerate(var_tokens_batch):
   368        64      14000.0    218.8      0.0          cur_bach_vars = []
   369        64   14667000.0 229171.9      0.4          xs = x_generate(len(batch_vars), temp_encoder[batch_num], all_nums_encoder_outputs[batch_num], problem_output[batch_num])
   370                                                   # get the generated variables
   371       192      56000.0    291.7      0.0          for variable in target_ordering_of_vars:
   372       128      32000.0    250.0      0.0              match = False
   373       270     123000.0    455.6      0.0              for var, x in zip(batch_vars, xs):
   374       142      24000.0    169.0      0.0                  if var == variable:
   375        71      22000.0    309.9      0.0                      cur_bach_vars.append(x)
   376        71       9000.0    126.8      0.0                      match = True
   377       128      20000.0    156.2      0.0              if not match:
   378        57       9000.0    157.9      0.0                  cur_bach_vars.append(empty)
   379        64     307000.0   4796.9      0.0          batch_all_vars.append(torch.stack(cur_bach_vars))
   380                                           
   381                                               # all_vars_embs: num_batches x num_vars x hidden_size
   382                                               # 0s if the var is not used in that observation
   383         1      57000.0  57000.0      0.0      all_vars_embs = torch.stack(batch_all_vars)
   384                                           
   385                                               
   386                                               # get the q vectors for each x
   387         1   23396000.0    2e+07      0.7      qs = x_to_q(temp_encoder, all_vars_embs)
   388                                           
   389                                               # number mask 
   390                                               # 0s where the numbers are from input, 1s where not in input
   391         1       1000.0   1000.0      0.0      num_mask = []
   392         1       3000.0   3000.0      0.0      max_num_size = max(num_size_batch) + len(generate_nums) 
   393        65       8000.0    123.1      0.0      for i in num_size_batch:
   394        64       8000.0    125.0      0.0          d = i + len(generate_nums) 
   395        64      67000.0   1046.9      0.0          num_mask.append([0] * d + [1] * (max_num_size - d))
   396         1     189000.0 189000.0      0.0      num_mask = torch.ByteTensor(num_mask)
   397                                               
   398                                               # for each batch, if there are variables in output lang [x,y,z] but group only has [x,y], mask would be [0,0,1]
   399         1          0.0      0.0      0.0      var_mask = []
   400                                               # first get variables in output lang 
   401                                               # var_list = [output_lang.variables for _ in target_batch]
   402                                               # for each equation group
   403        65      11000.0    169.2      0.0      for i, equ_group in enumerate(target_batch):
   404        64      11000.0    171.9      0.0          group_var_mask = []
   405        64     334000.0   5218.8      0.0          unique_in_equation = [output_lang.index2word[i] for i in list(set([el for arr in equ_group for el in arr]))]
   406       192      41000.0    213.5      0.0          for var in output_lang.variables:
   407                                                       # decoded = output_lang.index2word[var]
   408       128      28000.0    218.8      0.0              if var in unique_in_equation:
   409        71      19000.0    267.6      0.0                  group_var_mask.append(0)
   410                                                       else:
   411        57       9000.0    157.9      0.0                  group_var_mask.append(1)
   412        64      15000.0    234.4      0.0          var_mask.append(group_var_mask)
   413         1      12000.0  12000.0      0.0      var_mask = torch.ByteTensor(var_mask)
   414         1       2000.0   2000.0      0.0      addedQs = len(qs)
   415                                               # make sure to note that the updated encoder has the x vectors
   416                                           
   417                                           
   418                                               # index in the language where the special (operators) tokens end and input/output text begins
   419         1       1000.0   1000.0      0.0      num_start = output_lang.num_start
   420                                               # 
   421         1          0.0      0.0      0.0      all_outputs = []
   422         1       1000.0   1000.0      0.0      final_tokens = []
   423         2       1000.0    500.0      0.0      for equation_count in range(max_num_equations):
   424         2    1079000.0 539500.0      0.0          embeddings_stacks = [[] for _ in range(batch_size)]
   425         2      11000.0   5500.0      0.0          left_childs = [None for _ in range(batch_size)]
   426         2      26000.0  13000.0      0.0          current_equation_outputs = []
   427         2      16000.0   8000.0      0.0          current_equation = target[equation_count]
   428                                           
   429                                                   # make a TreeNode for each token
   430                                                   # node just has embedding and a left flag? 
   431         2     936000.0 468000.0      0.0          tempSplit = problem_output.split(1, dim=0)
   432                                                   # problem_output is q_0 for each token in equation, so use last one
   433         2    1119000.0 559500.0      0.0          node_stacks = [[TreeNode(_.unsqueeze(0))] for _ in qs.transpose(0,1)[equation_count]]
   434                                                   
   435        51      19000.0    372.5      0.0          for t in range(max_target_length):
   436                                           
   437                                                       # predict gets the encodings and embeddings for the current node 
   438                                                       #   num_score: batch_size x num_length
   439                                                       #       likliehood prediction of each number
   440                                                       #   op: batch_size x num_ops
   441                                                       #       likliehood of the operator tokens
   442                                                       #   current_embeddings: batch_size x 1 x hidden_size
   443                                                       #       goal vector (q) for the current node 
   444                                                       #   current_context: batch_size x 1 x hidden_size
   445                                                       #       context vector (c) for the subtree
   446                                                       #   embedding_weight: batch_size x num_length x hidden_size
   447                                                       #       embeddings of the generate and copy numbers
   448                                           
   449       100 2409674000.0    2e+07     72.0              num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(
   450        50      18000.0    360.0      0.0                  node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask, var_mask, all_vars_embs)
   451                                           
   452                                           
   453                                                       # this is mainly what we want to train
   454        49     882000.0  18000.0      0.0              outputs = torch.cat((op, num_score), 1)
   455        49      50000.0   1020.4      0.0              current_equation_outputs.append(outputs)
   456                                           
   457                                                       # target[t] is the equation character at index t for each batch
   458                                                       #    target[t] = 1 x num_batches
   459                                                       # outputs is the strength of operators or a number token
   460                                                       # num_stack_batch is the cooresponding num lists
   461                                                       # num_start is where non-operators begin
   462                                                       # unk is unknown token
   463                                                       # returns
   464                                                       #   for position t in each equation
   465                                                       #       target_t: actual equation value
   466                                                       #       generate_input: equation value if its an operator
   467                                                       # target_t, generate_input = generate_tree_input(target[equation_count][t].tolist(), outputs, nums_stack_batch, num_start, unk)
   468        49    6027000.0 123000.0      0.2              target_t, generate_input = generate_tree_input(current_equation[t].tolist(), outputs, nums_stack_batch, num_start, unk)
   469                                                       # target[t] = target_t
   470        49      25000.0    510.2      0.0              if USE_CUDA:
   471                                                           generate_input = generate_input.cuda()
   472                                           
   473                                                       # takes:
   474                                                       #     generate a left and right child node with a label
   475                                                       #     current_embeddings: q : batch_size x 1 x hidden_dim
   476                                                       #     generate_input: [operator tokens at position t]
   477                                                       #     current_context: c : batch_size x 1 x hidden_dim
   478                                                       # returns
   479                                                       #     l_child: batch_size x hidden_dim
   480                                                       #          hidden state h_l:
   481                                                       #     r_child: batch_size x hidden_dim
   482                                                       #          hidden state h_r:
   483                                                       #     node_label_ : batch_size x embedding_size 
   484                                                       #          basically the context vector (c)
   485                                                       # the node generation takes the first half of equations (10) and (11) 
   486        49   98257000.0    2e+06      2.9              left_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)
   487        49      45000.0    918.4      0.0              left_childs = []
   488      3234   11760000.0   3636.4      0.4              for idx, l, r, node_stack, i, o in zip(range(batch_size), left_child.split(1), right_child.split(1),
   489        49      12000.0    244.9      0.0                                                  node_stacks, target_t, embeddings_stacks):
   490                                                           # current_token = output_lang.ids_to_tokens([i])
   491                                                           # current_equation = output_lang.ids_to_tokens(target.transpose(0,1)[idx])
   492                                                           #print("at token", current_token, "in", current_equation)
   493                                                           #print("current node_stack length", len(node_stack))
   494                                                           # for 
   495                                                           #   batch_num
   496                                                           #   the left child: h_l 
   497                                                           #   the right child: h_r
   498      3136     662000.0    211.1      0.0                  if len(node_stack) != 0:
   499      1830    1699000.0    928.4      0.1                      node = node_stack.pop()
   500                                                               #print("removed last from node_stack, now", len(node_stack), "elems")
   501                                                           else:
   502      1306     264000.0    202.1      0.0                      left_childs.append(None)
   503      1306     113000.0     86.5      0.0                      continue
   504                                           
   505                                                           # i is the num in language of where that specific language token is
   506                                                           # if i is an operator
   507      1830    6065000.0   3314.2      0.2                  if i < num_start:
   508                                                               #print(current_token, "is an operator, making a left and right node")
   509                                                               # make a left and right tree node
   510      1473     930000.0    631.4      0.0                      node_stack.append(TreeNode(r))
   511      1473     932000.0    632.7      0.0                      node_stack.append(TreeNode(l, left_flag=True))
   512                                                               # save the embedding of the operator 
   513                                                               # terminal means a leaf node
   514      1473    5142000.0   3490.8      0.2                      o.append(TreeEmbedding(node_label[idx].unsqueeze(0), False))
   515                                                               #print("saving node embedding to o (non terminal node), and r, and l to node_stack. o now of size", len(o), "node_stack of size", len(node_stack))
   516                                                           else:
   517                                                               # otherwise its either a number from the input equation or a copy number
   518                                                               # we have a list (o) of the current nodes in the tree
   519                                                               # if we have a leaf node at the top of the stack, get it.
   520                                                               # next element in the stack must be an operator, so get it 
   521                                                               # and combine the new node, operator, and other element
   522                                           
   523                                                               # current_nums_embedding: batch_size x num_length x hidden_size
   524                                                               # current_num = num_embedding of the number selected
   525       357    5006000.0  14022.4      0.1                      current_num = current_nums_embeddings[idx, i - num_start].unsqueeze(0)
   526                                                               # while there are tokens in the embedding stack and the last element IS a leaf node
   527       642     369000.0    574.8      0.0                      while len(o) > 0 and o[-1].terminal:
   528                                                                   #print("terminal element in o, getting terminal element and operator, and merging")
   529                                                                   # get the two elements from it
   530       285    2843000.0   9975.4      0.1                          sub_stree = o.pop()
   531       285     215000.0    754.4      0.0                          op = o.pop()
   532                                                                   # contains equation (13)
   533                                                                   # this combines a left and right tree along with a node
   534       285   38692000.0 135761.4      1.2                          current_num = merge(op.embedding, sub_stree.embedding, current_num)
   535                                                                   #print('merged. o now of size', len(o))
   536                                                               # then re-add the node back to the stack
   537                                                               #print("adding current_num to o (terminal node)")
   538       357     312000.0    873.9      0.0                      o.append(TreeEmbedding(current_num, True))
   539      1830     577000.0    315.3      0.0                  if len(o) > 0 and o[-1].terminal:
   540                                                               #print("terminal element in o, adding to left child")
   541                                                               # left_childs is a running vector of the sub tree embeddings "t" 
   542                                                               # need this for generation of the right q
   543       357     135000.0    378.2      0.0                      left_childs.append(o[-1].embedding)
   544                                                           else:
   545      1473     363000.0    246.4      0.0                      left_childs.append(None)
   546                                                   # tree has been built. now predict the = " " token
   547         2   45583000.0    2e+07      1.4          num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(
   548         1       1000.0   1000.0      0.0              node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask, var_mask, all_vars_embs)
   549         1      15000.0  15000.0      0.0          final_token = torch.cat((op, num_score), 1)
   550         1       2000.0   2000.0      0.0          final_tokens.append(final_token)
   551         1     178000.0 178000.0      0.0          all_outputs.append(torch.stack(current_equation_outputs, dim = 1))
   552                                           
   553                                               # final tokens:
   554                                               preds_final_tokens_all = torch.stack(final_tokens, dim = 1)
   555                                               preds_final_tokens_all_flattened = preds_final_tokens_all.view(-1, preds_final_tokens_all.size(-1))
   556                                           
   557                                               # batch solutons: 
   558                                               # mask for solutions
   559                                               solution_batch_mask = []
   560                                               final_solutions = []
   561                                               for i, batch in enumerate(solution_batch):
   562                                                   if len(batch) < max_num_equations:
   563                                                       temp_batch = batch + [0 for _ in range(max_num_equations - len(batch))]
   564                                                       final_solutions.append(temp_batch)
   565                                                       solution_batch_mask.append([1 for _ in range(len(batch))] + [0 for _ in range(max_num_equations - len(batch))])
   566                                                   else:
   567                                                       final_solutions.append(batch)
   568                                                       solution_batch_mask.append([1 for _ in range(len(batch))])
   569                                           
   570                                               print('equation output')
   571                                               same = 0
   572                                               lengths = 0 
   573                                               for i, batch in enumerate(preds_final_tokens_all):
   574                                                   for j, prediction in enumerate(batch):
   575                                                       if final_solutions[i][j] != 0:
   576                                                           lengths += 1
   577                                                           print(f"    preds for batch {i}, equation {j}: {output_lang.index2word[torch.argmax(prediction).item()]}, actual: {output_lang.index2word[final_solutions[i][j]]}")
   578                                                           if output_lang.index2word[torch.argmax(prediction).item()] == output_lang.index2word[final_solutions[i][j]]:
   579                                                               same += 1
   580                                           
   581                                           
   582                                               # loss the equation result
   583                                               solution_batch_mask = torch.ByteTensor(solution_batch_mask)
   584                                               final_solutions_flattened = torch.LongTensor(final_solutions).view(-1)
   585                                           
   586                                               # solutions_all_loss
   587                                               solutions_raw_loss = torch.nn.CrossEntropyLoss(reduction='none')(preds_final_tokens_all_flattened, final_solutions_flattened.to(device) )
   588                                               
   589                                               solutions_loss = solutions_raw_loss.view(preds_final_tokens_all.size(0), preds_final_tokens_all.size(1))
   590                                               # apply mask to loss
   591                                               solutions_loss_final = solutions_loss * solution_batch_mask.float().to(device)
   592                                               solutions_loss_final = solutions_loss_final.sum()
   593                                               print('solution loss', solutions_loss_final)
   594                                           
   595                                               # all_node_outputs:  
   596                                               #  for each equation 
   597                                               #    for each token in equation: 
   598                                               #      the current scoring of nums for each batch
   599                                               # 
   600                                               # transform to 
   601                                               # all_node_outputs2: for each batch:
   602                                               #   the current scoring of nums for each token in equation
   603                                               # = batch_size x max_len x num_nums
   604                                               all_outputs_stacked = torch.stack(all_outputs, dim=-1)
   605                                           
   606                                               # target: num_equations x num_tokens x batch_size
   607                                               target2 = target.transpose(0, 1).contiguous()
   608                                               target3 = target2.transpose(1, 2).contiguous()
   609                                               # actual
   610                                               # target4: batch_size x max_len x num_equations
   611                                               target4 = target3.transpose(0,1).contiguous()
   612                                           
   613                                               if USE_CUDA:
   614                                                   # all_leafs = all_leafs.cuda()
   615                                                   all_node_outputs2 = all_outputs
   616                                                   target = target.cuda()
   617                                           
   618                                           
   619                                               target_length_filled = []
   620                                               for problem in target_length:
   621                                                   current_num_equations = len(problem)
   622                                                   if current_num_equations < max_num_equations:
   623                                                       problem += [0 for _ in range(max_num_equations - current_num_equations)]
   624                                                   target_length_filled.append(problem)
   625                                               # target_length_filled = torch.Tensor(target_length_filled)
   626                                               # loss = masked_cross_entropy(all_node_outputs2, target, target_length)
   627                                               # loss the number of equations
   628                                               actual_num_x = torch.LongTensor([(len(i) - 1) if i != [] else 0 for i in var_tokens_batch])
   629                                               num_x_loss = torch.nn.CrossEntropyLoss()(num_x, actual_num_x.to(device) )
   630                                               # print('num x loss', num_x_loss)
   631                                               # print('number of equations/variables')
   632                                               # for i, batch in enumerate(num_x):
   633                                               #     print(f"    preds for batch {i}: {batch} equations. Actual: {actual_num_x[i]}")
   634                                               # print('     mse loss', loss)
   635                                           
   636                                               target_mask1 = 1 - torch.ByteTensor(target_mask)
   637                                               target_mask_tensor = target_mask1.transpose(1,2)
   638                                               # for batch in target_mask:   
   639                                               #     batch_equs = []
   640                                               #     for equation in batch:
   641                                               #         batch_equs.append(torch.Tensor(equation))
   642                                               #     batch_stack = torch.stack(batch_equs, dim=0)
   643                                           
   644                                               # target_mask_stacked = torch.stack([torch.Tensor(i) for i in target_mask], dim=-1)
   645                                           
   646                                               print('token lists of')
   647                                               if solutions_loss_final > 100000:
   648                                                   print('solutions high')
   649                                                   raise Exception('high') 
   650                                               if  num_x_loss > 100000:
   651                                                   raise Exception('num x high') 
   652                                               loss = solutions_loss_final + num_x_loss
   653                                               # equation_loss = solutions_loss_final
   654                                               for i in range(max_num_equations):
   655                                                   # target4 = batch_size x max_len x num_equations
   656                                                   # equation_target = batch_size x max_len 
   657                                                   equation_target = target4[..., i]
   658                                                   equation_mask = target_mask_tensor[..., i]
   659                                                   target_flattened = equation_target.view(-1)
   660                                                   # equation_mask_flattened = equation_mask.view(-1)
   661                                                   # for each batch 
   662                                                   target_tokens = []
   663                                                   for actuals in equation_target:
   664                                                       # print("target", actuals)
   665                                                       target_tokens.append([output_lang.index2word[_] for _ in actuals])
   666                                                   # print("target combined", target_tokens)
   667                                                   # all_outputs_stacked = batch_size x max_len x vocab_len x num_equations 
   668                                                   # predictions = batch_size x max_len x vocab_len
   669                                                   predictions = all_outputs_stacked[..., i]
   670                                                   pred_distribution = predictions.log_softmax(-1)
   671                                                   for j, batch in enumerate(predictions):
   672                                                       eqn_preds = []
   673                                                       for token in batch:
   674                                                           eqn_preds.append(output_lang.index2word[torch.argmax(token).item()])
   675                                                       print(f"    batch {j}, equation {i}" )
   676                                                       print(f"        prkediction: {eqn_preds[0:target_length[j][i]]}")
   677                                                       print(f"        actual: {target_tokens[j][0:target_length[j][i]]}")
   678                                                       for k in range(len(eqn_preds[0:target_length[j][i]])):
   679                                                           if eqn_preds[0:target_length[j][i]][k] == target_tokens[j][0:target_length[j][i]][k]:
   680                                                               same += 1
   681                                                           lengths += 1
   682                                                       # print(f'{same} so far')
   683                                                   # preds_flattened = predictions.view(-1, predictions.size(-1))
   684                                                   preds_flattened = pred_distribution.view(-1, predictions.size(-1))
   685                                           
   686                                                   tempLoss = torch.nn.CrossEntropyLoss(reduction='none')(preds_flattened, target_flattened.to(device))
   687                                                   # print('tempLoss', tempLoss)
   688                                                   # reshape loss:
   689                                                   tempLoss2 = tempLoss.view(equation_target.size(0), equation_target.size(1))
   690                                                   # apply mask to loss
   691                                                   tempLoss3 = tempLoss2 * equation_mask.float().to(device)
   692                                           
   693                                                   # equation_loss += tempLoss3.sum() 
   694                                                   print(f'equ {i} loss, {tempLoss3.sum()}')
   695                                                   # for lossT, token in zip(tempLoss3, target_flattened):
   696                                                   #     for i, l in enumerate(lossT):
   697                                                   #         print(f"batch: {i}")
   698                                                   #         print(f"    token {output_lang.index2word[token]} loss: {l}")
   699                                                   #         if l > 900000:
   700                                                   #             print("ANOMOLY")
   701                                                   #             print('prediction list', predictions[i])
   702                                                   if loss is None:
   703                                                       loss = tempLoss3.sum()
   704                                                   else:
   705                                                       loss += tempLoss3.sum()
   706                                           
   707                                               # full loss = equation loss + number of equations loss + solutions loss
   708                                               # loss = equation_loss + num_x_loss
   709                                               print('total loss', loss)
   710                                               loss.backward()
   711                                           
   712                                               # make_dot(loss).render("loss")
   713                                           
   714                                           
   715                                               return loss.item(), same/lengths

  3.35 seconds - /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py:254 - train_tree
