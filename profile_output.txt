Timer unit: 1e-09 s

Total time: 2.75791 s
File: /Users/home/school/thesis/TESTING/math_seq2tree/src/models.py
Function: forward at line 268

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   268                                               @line_profiler.profile  
   269                                               def forward(self, node_stacks, left_childs, encoder_outputs, num_pades, padding_hidden, seq_mask, mask_nums):
   270                                                   # node_stacks: [TreeNodes] for each node containing the hidden state for the node
   271                                                   # left_childs: [] of 
   272                                                   # encoder_outputs: token embeddings: max_len x num_batches x hidden state 
   273                                                   # num_pads:  embeddings of the numbers: num_batches x num_size x hidden_size 
   274                                                   # padding_hidden: 0s of hidden_size
   275                                                   # seq_mask: num_batches x seq_len 
   276                                                   # mask_nums: 0s where the num in the nums emb come from input text. 
   277                                                   # this aligns with the num_pades. num_batches x num_size
   278                                           
   279                                                   # let num_length = 2 + len(numbers)
   280                                           
   281      1606     947000.0    589.7      0.0          current_embeddings1 = []
   282                                           
   283                                                   # for each stack of tokens 
   284                                                   # 2 batches = 2 stacks
   285      4241    1211000.0    285.5      0.0          for st in node_stacks:
   286                                                       # not sure why would be zero. it's initialized w/ single num each 
   287                                                       # if it is zero, let that token's embedding be the zero embedding
   288      2635    1150000.0    436.4      0.0              if len(st) == 0:
   289       438     131000.0    299.1      0.0                  current_embeddings1.append(padding_hidden)
   290                                                       else:
   291                                                           # use embedding from the last node in the stack
   292      2197     567000.0    258.1      0.0                  current_node = st[-1]
   293      2197     905000.0    411.9      0.0                  current_embeddings1.append(current_node.embedding)
   294                                                   
   295                                                   # current_embeddings1 = the embedding of the last node in the stack
   296                                           
   297      1606     303000.0    188.7      0.0          current_node_temp = []
   298      4241    3450000.0    813.5      0.1          for l, c in zip(left_childs, current_embeddings1):
   299                                                       # l = left child
   300                                                       # c = embedding of parent node
   301                                                       # second half of equation (10)
   302      2635     568000.0    215.6      0.0              if l is None:
   303                                                           # if not left child (this is a leaf)
   304                                                           # nodes context vector 1 x hidden_dim
   305      1884   27809000.0  14760.6      1.0                  c = self.dropout(c)
   306      1884  124321000.0  65987.8      4.5                  g = torch.tanh(self.concat_l(c))
   307      1884  114937000.0  61006.9      4.2                  t = torch.sigmoid(self.concat_lg(c))
   308      1884    5541000.0   2941.1      0.2                  current_node_temp.append(g * t)
   309                                                       else:
   310                                                           # second half of equation (11)
   311       751   12639000.0  16829.6      0.5                  ld = self.dropout(l)
   312       751    8135000.0  10832.2      0.3                  c = self.dropout(c)
   313       751   78516000.0 104548.6      2.8                  g = torch.tanh(self.concat_r(torch.cat((ld, c), 1)))
   314       751   73556000.0  97944.1      2.7                  t = torch.sigmoid(self.concat_rg(torch.cat((ld, c), 1)))
   315       751    2498000.0   3326.2      0.1                  current_node_temp.append(g * t)
   316                                           
   317                                           
   318                                                   # batch_size x 1 x hidden_dim
   319      1606   15963000.0   9939.6      0.6          current_node = torch.stack(current_node_temp)
   320                                                   # this is the q of the current subtree
   321      1606   18361000.0  11432.8      0.7          current_embeddings = self.dropout(current_node)
   322                                           
   323                                                   # this gets the score calculation and relation between each 
   324                                                   # encoded token
   325                                                   # this is the a in equation (6)
   326      1606 1163296000.0 724343.7     42.2          current_attn = self.attn(current_embeddings.transpose(0, 1), encoder_outputs, seq_mask)
   327                                                   # the encoder_outputs are the h
   328                                           
   329                                                   # this is the context vector 
   330                                                   # equation (6)
   331      1606   32335000.0  20133.9      1.2          current_context = current_attn.bmm(encoder_outputs.transpose(0, 1))  # B x 1 x N
   332                                           
   333                                                   # the information to get the current quantity
   334      1606    1258000.0    783.3      0.0          batch_size = current_embeddings.size(0)
   335                                           
   336                                           
   337                                                   # predict the output (this node corresponding to output(number or operator)) with PADE
   338                                           
   339      1606    4524000.0   2816.9      0.2          repeat_dims = [1] * self.embedding_weight.dim()
   340      1606     563000.0    350.6      0.0          repeat_dims[0] = batch_size
   341                                                   # self.embedding_weight:
   342                                                   #   1 x 2 x hidden_dim
   343                                                   #   this is the amount to weight the each hidden dim for 2 things
   344                                                   #       maybe the current context, and the leaf context? 
   345                                           
   346                                           
   347                                                   # repeat the embedding weight for each batch
   348                                                   # batch_size x 2 x hidden_dim
   349      1606   19836000.0  12351.2      0.7          embedding_weight1 = self.embedding_weight.repeat(*repeat_dims)  # B x input_size x N
   350                                           
   351                                                   # batch_size x (2 + number of numbers we have encodings for) x hidden_dim
   352                                                   # batch_size is the embeddings of the numbers
   353                                                   #   batch_size x nums_count x hidden_dim
   354      1606   13232000.0   8239.1      0.5          embedding_weight = torch.cat((embedding_weight1, num_pades), dim=1)  # B x O x N
   355                                           
   356                                           
   357                                                   # get the embedding of a leaf
   358                                                   # embedding of a leaf is the concatenation of 
   359                                                   #   the node embedding and the embedding after attention
   360                                                   # this is the [q c]
   361                                                   # batch_size x 1 x 2*hidden_dim 
   362      1606    5704000.0   3551.7      0.2          leaf_input1 = torch.cat((current_node, current_context), 2)
   363                                                   # batch_size x 2*hidden_dim 
   364      1606    3461000.0   2155.0      0.1          leaf_input = leaf_input1.squeeze(1)
   365      1606   24729000.0  15397.9      0.9          leaf_input = self.dropout(leaf_input)
   366                                           
   367                                                   # p_leaf = nn.functional.softmax(self.is_leaf(leaf_input), 1)
   368                                                   # max pooling the embedding_weight
   369      1606   35168000.0  21897.9      1.3          embedding_weight_ = self.dropout(embedding_weight)
   370                                           
   371                                           
   372                                                   # get the scores between the leaves,  
   373                                                   # leaf_input is the embedding of a leaf
   374                                                   # TODO
   375                                                   # embedding_weight is the weight matrix of scaling the input hidden dims? (need to conform this is true)
   376                                                   # mask_nums is batch_size x max_nums_length and is 0 where the num comes from text
   377                                                   # equation (7) done here
   378                                                   # this is the log likliehood of generating token y from the specified vocab
   379                                                   #   doesnt seem like the full vocab is being used, only
   380      1606  930646000.0 579480.7     33.7          num_score = self.score(leaf_input.unsqueeze(1), embedding_weight_, mask_nums)
   381                                           
   382                                                   # get the predicted operation (classification)
   383                                                   # batch_size x num_ops 
   384      1606   30912000.0  19247.8      1.1          op = self.ops(leaf_input)
   385                                           
   386                                                   # return p_leaf, num_score, op, current_embeddings, current_attn
   387                                           
   388                                                   # this returns
   389                                                   #   num_score: batch_size x num_length
   390                                                   #       score values of each number
   391                                                   #   op: operation list
   392                                                   #       batch_size x num_ops
   393                                                   #   current_node: q : batch_size x 1 x hidden_size
   394                                                   #       goal vector for the subtree
   395                                                   #   current_context: c : batch_size x 1 x hidden_size
   396                                                   #       context vector for the subtree
   397                                                   #   embedding_weight : batch_size x num_length x hidden_size
   398                                                   #       number embeddings
   399      1606     737000.0    458.9      0.0          return num_score, op, current_node, current_context, embedding_weight

Total time: 5.27465 s
File: /Users/home/school/thesis/TESTING/math_seq2tree/src/train_and_evaluate.py
Function: evaluate_tree at line 491

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   491                                           @line_profiler.profile
   492                                           def evaluate_tree(input_batch, input_length, generate_nums, encoder, predict, generate, merge, output_lang, num_pos,
   493                                                             beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):
   494                                           
   495        30     607000.0  20233.3      0.0      seq_mask = torch.ByteTensor(1, input_length).fill_(0)
   496                                               # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)
   497        30     433000.0  14433.3      0.0      input_var = torch.LongTensor(input_batch).unsqueeze(1)
   498                                           
   499        30      82000.0   2733.3      0.0      num_mask = torch.ByteTensor(1, len(num_pos) + len(generate_nums)).fill_(0)
   500                                           
   501                                               # Set to not-training mode to disable dropout
   502        30    1323000.0  44100.0      0.0      encoder.eval()
   503        30    1657000.0  55233.3      0.0      predict.eval()
   504        30     842000.0  28066.7      0.0      generate.eval()
   505        30     490000.0  16333.3      0.0      merge.eval()
   506                                           
   507        30    1867000.0  62233.3      0.0      padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)
   508                                           
   509        30       3000.0    100.0      0.0      batch_size = 1
   510                                           
   511        30      12000.0    400.0      0.0      if USE_CUDA:
   512                                                   input_var = input_var.cuda()
   513                                                   seq_mask = seq_mask.cuda()
   514                                                   padding_hidden = padding_hidden.cuda()
   515                                                   num_mask = num_mask.cuda()
   516                                               # Run words through encoder
   517                                           
   518        30  368700000.0    1e+07      7.0      encoder_outputs, problem_output = encoder(input_var, [input_length])
   519                                           
   520                                               # Prepare input and output variables
   521        30     620000.0  20666.7      0.0      node_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]
   522                                           
   523        30      26000.0    866.7      0.0      num_size = len(num_pos)
   524        60    6675000.0 111250.0      0.1      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,
   525        30       5000.0    166.7      0.0                                                                encoder.hidden_size)
   526        30      27000.0    900.0      0.0      num_start = output_lang.num_start
   527                                               # B x P x N
   528        30      49000.0   1633.3      0.0      embeddings_stacks = [[] for _ in range(batch_size)]
   529        30      28000.0    933.3      0.0      left_childs = [None for _ in range(batch_size)]
   530                                           
   531                                               # evaulation uses beam search
   532                                               # key is how the beams are compared
   533        30     500000.0  16666.7      0.0      beams = [TreeBeam(0.0, node_stacks, embeddings_stacks, left_childs, [])]
   534                                           
   535       363     152000.0    418.7      0.0      for t in range(max_length):
   536       358   99875000.0 278980.4      1.9          current_beams = []
   537      2028    1154000.0    569.0      0.0          while len(beams) > 0:
   538      1670    1316000.0    788.0      0.0              b = beams.pop()
   539      1670     891000.0    533.5      0.0              if len(b.node_stack[0]) == 0:
   540       203      52000.0    256.2      0.0                  current_beams.append(b)
   541       203      28000.0    137.9      0.0                  continue
   542                                                       # left_childs = torch.stack(b.left_childs)
   543      1467     311000.0    212.0      0.0              left_childs = b.left_childs
   544                                           
   545      2934 2164753000.0 737816.3     41.0              num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(
   546      1467     597000.0    407.0      0.0                  b.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden,
   547      1467     257000.0    175.2      0.0                  seq_mask, num_mask)
   548                                           
   549                                                       # leaf = p_leaf[:, 0].unsqueeze(1)
   550                                                       # repeat_dims = [1] * leaf.dim()
   551                                                       # repeat_dims[1] = op.size(1)
   552                                                       # leaf = leaf.repeat(*repeat_dims)
   553                                                       #
   554                                                       # non_leaf = p_leaf[:, 1].unsqueeze(1)
   555                                                       # repeat_dims = [1] * non_leaf.dim()
   556                                                       # repeat_dims[1] = num_score.size(1)
   557                                                       # non_leaf = non_leaf.repeat(*repeat_dims)
   558                                                       #
   559                                                       # p_leaf = torch.cat((leaf, non_leaf), dim=1)
   560      1467   24255000.0  16533.7      0.5              out_score = nn.functional.log_softmax(torch.cat((op, num_score), dim=1), dim=1)
   561                                           
   562                                                       # out_score = p_leaf * out_score
   563                                           
   564                                                       # topv:
   565                                                       #   largest elements in the out_score
   566                                                       # topi:
   567                                                       #   indexes of the largest elements 
   568      1467   14286000.0   9738.2      0.3              topv, topi = out_score.topk(beam_size)
   569                                           
   570                                                       # is_leaf = int(topi[0])
   571                                                       # if is_leaf:
   572                                                       #     topv, topi = op.topk(1)
   573                                                       #     out_token = int(topi[0])
   574                                                       # else:
   575                                                       #     topv, topi = num_score.topk(1)
   576                                                       #     out_token = int(topi[0]) + num_start
   577                                           
   578                                                       # for the largest element, and its index
   579      8802   98153000.0  11151.2      1.9              for tv, ti in zip(topv.split(1, dim=1), topi.split(1, dim=1)):
   580      7335   34400000.0   4689.8      0.7                  current_node_stack = copy_list(b.node_stack)
   581      7335    1307000.0    178.2      0.0                  current_left_childs = []
   582      7335   37592000.0   5125.0      0.7                  current_embeddings_stacks = copy_list(b.embedding_stack)
   583      7335  147963000.0  20172.2      2.8                  current_out = copy.deepcopy(b.out)
   584                                           
   585                                                           # the predicted token is that of the highest score relation
   586      7335   10049000.0   1370.0      0.2                  out_token = int(ti)
   587                                                           # save token 
   588      7335    2078000.0    283.3      0.0                  current_out.append(out_token)
   589                                           
   590      7335    3794000.0    517.2      0.1                  node = current_node_stack[0].pop()
   591                                           
   592                                                           # if the predicted token is an operator
   593      7335    1600000.0    218.1      0.0                  if out_token < num_start:
   594                                                               # this is the token to generate l and r from
   595      3336   25326000.0   7591.7      0.5                      generate_input = torch.LongTensor([out_token])
   596      3336    1073000.0    321.6      0.0                      if USE_CUDA:
   597                                                                   generate_input = generate_input.cuda()
   598                                                               # get the left and right children and current label
   599      3336 1484676000.0 445046.8     28.1                      left_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)
   600                                           
   601      3336    8800000.0   2637.9      0.2                      current_node_stack[0].append(TreeNode(right_child))
   602      3336    4205000.0   1260.5      0.1                      current_node_stack[0].append(TreeNode(left_child, left_flag=True))
   603                                           
   604      3336   31725000.0   9509.9      0.6                      current_embeddings_stacks[0].append(TreeEmbedding(node_label[0].unsqueeze(0), False))
   605                                                           else:
   606                                                               # predicted token is a number
   607                                                               # get the token embedding - embedding of either the generate num or copy num
   608      3999   30596000.0   7650.9      0.6                      current_num = current_nums_embeddings[0, out_token - num_start].unsqueeze(0)
   609                                                               
   610                                                               # if we are a right node (there is a left node and operator)
   611      5784    4650000.0    803.9      0.1                      while len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:
   612      1785    1030000.0    577.0      0.0                          sub_stree = current_embeddings_stacks[0].pop()
   613      1785     826000.0    462.7      0.0                          op = current_embeddings_stacks[0].pop()
   614      1785  368579000.0 206486.8      7.0                          current_num = merge(op.embedding, sub_stree.embedding, current_num)
   615                                                               # save node (or subtree) to the embeddings list
   616      3999    4671000.0   1168.0      0.1                      current_embeddings_stacks[0].append(TreeEmbedding(current_num, True))
   617      7335    4842000.0    660.1      0.1                  if len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:
   618      3999    1619000.0    404.9      0.0                      current_left_childs.append(current_embeddings_stacks[0][-1].embedding)
   619                                                           else:
   620      3336    1324000.0    396.9      0.0                      current_left_childs.append(None)
   621                                                           # the beam "score" is the sum of the associations 
   622     14670  264856000.0  18054.3      5.0                  current_beams.append(TreeBeam(b.score+float(tv), current_node_stack, current_embeddings_stacks,
   623      7335    1512000.0    206.1      0.0                                                current_left_childs, current_out))
   624                                                   # order beam by highest to lowest
   625       358    3522000.0   9838.0      0.1          beams = sorted(current_beams, key=lambda x: x.score, reverse=True)
   626       358     350000.0    977.7      0.0          beams = beams[:beam_size]
   627       358      72000.0    201.1      0.0          flag = True
   628      2148     634000.0    295.2      0.0          for b in beams:
   629      1790     619000.0    345.8      0.0              if len(b.node_stack[0]) != 0:
   630      1459     198000.0    135.7      0.0                  flag = False
   631       358     117000.0    326.8      0.0          if flag:
   632        25      11000.0    440.0      0.0              break
   633                                           
   634        30      12000.0    400.0      0.0      return beams[0].out

Total time: 8.1991 s
File: /Users/home/school/thesis/TESTING/math_seq2tree/src/train_and_evaluate.py
Function: train_tree at line 257

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   257                                           @line_profiler.profile
   258                                           def train_tree(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, generate_nums,
   259                                                          encoder, predict, generate, merge, encoder_optimizer, predict_optimizer, generate_optimizer,
   260                                                          merge_optimizer, output_lang, num_pos, english=False):
   261                                               # input_batch: padded inputs
   262                                               # input_length: length of the inputs (without padding)
   263                                               # target_batch: padded outputs
   264                                               # target_length: length of the outputs (without padding)
   265                                               # num_stack_batch: the corresponding nums lists
   266                                               # num_size_batch: number of numbers from the input text
   267                                               # generate_nums: numbers to generate
   268                                           
   269                                               # num_pos: positions of the numbers lists
   270                                           
   271                                           
   272                                               # sequence mask for attention
   273                                               # 0s where in input, 1s where not in input
   274        15      13000.0    866.7      0.0      seq_mask = []
   275        15      43000.0   2866.7      0.0      max_len = max(input_length)
   276       135      34000.0    251.9      0.0      for i in input_length:
   277       120     690000.0   5750.0      0.0          seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])
   278        15     750000.0  50000.0      0.0      seq_mask = torch.ByteTensor(seq_mask)
   279                                           
   280                                               # number mask 
   281                                               # 0s where the numbers are from input, 1s where not in input
   282        15       3000.0    200.0      0.0      num_mask = []
   283        15      18000.0   1200.0      0.0      max_num_size = max(num_size_batch) + len(generate_nums)
   284       135      30000.0    222.2      0.0      for i in num_size_batch:
   285       120      37000.0    308.3      0.0          d = i + len(generate_nums)
   286       120      91000.0    758.3      0.0          num_mask.append([0] * d + [1] * (max_num_size - d))
   287        15     123000.0   8200.0      0.0      num_mask = torch.ByteTensor(num_mask)
   288                                           
   289        15      21000.0   1400.0      0.0      unk = output_lang.word2index["UNK"]
   290                                           
   291                                               # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)
   292        15     529000.0  35266.7      0.0      input_var = torch.LongTensor(input_batch).transpose(0, 1)
   293        15     158000.0  10533.3      0.0      target = torch.LongTensor(target_batch).transpose(0, 1)
   294                                           
   295        15    1234000.0  82266.7      0.0      padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)
   296        15       6000.0    400.0      0.0      batch_size = len(input_length)
   297                                           
   298        15     842000.0  56133.3      0.0      encoder.train()
   299        15    1146000.0  76400.0      0.0      predict.train()
   300        15     595000.0  39666.7      0.0      generate.train()
   301        15     341000.0  22733.3      0.0      merge.train()
   302                                           
   303        15       5000.0    333.3      0.0      if USE_CUDA:
   304                                                   input_var = input_var.cuda()
   305                                                   seq_mask = seq_mask.cuda()
   306                                                   padding_hidden = padding_hidden.cuda()
   307                                                   num_mask = num_mask.cuda()
   308                                           
   309                                               # Zero gradients of both optimizers
   310        15   16040000.0    1e+06      0.2      encoder_optimizer.zero_grad()
   311        15    9553000.0 636866.7      0.1      predict_optimizer.zero_grad()
   312        15    9231000.0 615400.0      0.1      generate_optimizer.zero_grad()
   313        15    6143000.0 409533.3      0.1      merge_optimizer.zero_grad()
   314                                               # Run words through encoder
   315                                           
   316                                               # embedding + dropout layer
   317                                               # encoder_outputs: num_batches x 512 q_0 vector
   318                                               # problem_output: max_length x num_batches x hidden_size
   319        15 1535577000.0    1e+08     18.7      encoder_outputs, problem_output = encoder(input_var, input_length)
   320                                               # Prepare input and output variables
   321                                               
   322                                               # make a TreeNode for each token
   323                                               # node just has embedding and a left flag? 
   324        15     486000.0  32400.0      0.0      tempSplit = problem_output.split(1, dim=0)
   325                                               # problem_output is q_0 for each token in equation, so use last one
   326        15     375000.0  25000.0      0.0      node_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]
   327                                           
   328        15      32000.0   2133.3      0.0      max_target_length = max(target_length)
   329                                           
   330        15       3000.0    200.0      0.0      all_node_outputs = []
   331                                               # all_leafs = []
   332                                           
   333                                               # array of len of numbers that must be copied from the input text
   334        15      62000.0   4133.3      0.0      copy_num_len = [len(_) for _ in num_pos]
   335                                               # max nums to copy
   336        15       7000.0    466.7      0.0      num_size = max(copy_num_len)
   337                                           
   338                                               # num_batches x num_size x hidden_size that correspond to the embeddings of the numbers
   339        30   23326000.0 777533.3      0.3      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size,
   340        15       6000.0    400.0      0.0                                                                encoder.hidden_size)
   341                                           
   342                                           
   343                                               # index in the language where the special (operators) tokens end and input/output text begins
   344        15      22000.0   1466.7      0.0      num_start = output_lang.num_start
   345                                               # 
   346        15      71000.0   4733.3      0.0      embeddings_stacks = [[] for _ in range(batch_size)]
   347                                               # 
   348        15      32000.0   2133.3      0.0      left_childs = [None for _ in range(batch_size)]
   349       154      47000.0    305.2      0.0      for t in range(max_target_length):
   350                                           
   351                                                   # predict gets the encodings and embeddings for the current node 
   352                                                   #   num_score: batch_size x num_length
   353                                                   #       likliehood prediction of each number
   354                                                   #   op: batch_size x num_ops
   355                                                   #       likliehood of the operator tokens
   356                                                   #   current_embeddings: batch_size x 1 x hidden_size
   357                                                   #       goal vector (q) for the current node 
   358                                                   #   current_context: batch_size x 1 x hidden_size
   359                                                   #       context vector (c) for the subtree
   360                                                   #   embedding_weight: batch_size x num_length x hidden_size
   361                                                   #       embeddings of the generate and copy numbers
   362                                           
   363       278  652710000.0    2e+06      8.0          num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(
   364       139      50000.0    359.7      0.0              node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask)
   365                                           
   366                                           
   367                                                   # this is mainly what we want to train
   368       139     963000.0   6928.1      0.0          outputs = torch.cat((op, num_score), 1)
   369       139     103000.0    741.0      0.0          all_node_outputs.append(outputs)
   370                                           
   371                                                   # target[t] is the equation character at index t for each batch
   372                                                   #    target[t] = 1 x num_batches
   373                                                   # outputs is the strength of operators or a number token
   374                                                   # num_stack_batch is the cooresponding num lists
   375                                                   # num_start is where non-operators begin
   376                                                   # unk is unknown token
   377                                                   # returns
   378                                                   #   for position t in each equation
   379                                                   #       target_t: actual equation value
   380                                                   #       generate_input: equation value if its an operator
   381       139    7160000.0  51510.8      0.1          target_t, generate_input = generate_tree_input(target[t].tolist(), outputs, nums_stack_batch, num_start, unk)
   382       139     771000.0   5546.8      0.0          target[t] = target_t
   383       139      70000.0    503.6      0.0          if USE_CUDA:
   384                                                       generate_input = generate_input.cuda()
   385                                           
   386                                                   # takes:
   387                                                   #     generate a left and right child node with a label
   388                                                   #     current_embeddings: q : batch_size x 1 x hidden_dim
   389                                                   #     generate_input: [operator tokens at position t]
   390                                                   #     current_context: c : batch_size x 1 x hidden_dim
   391                                                   # returns
   392                                                   #     l_child: batch_size x hidden_dim
   393                                                   #          hidden state h_l:
   394                                                   #     r_child: batch_size x hidden_dim
   395                                                   #          hidden state h_r:
   396                                                   #     node_label_ : batch_size x embedding_size 
   397                                                   #          basically the context vector (c)
   398                                                   # the node generation takes the first half of equations (10) and (11) 
   399       139  225465000.0    2e+06      2.7          left_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)
   400       139      94000.0    676.3      0.0          left_childs = []
   401      1446   13827000.0   9562.2      0.2          for idx, l, r, node_stack, i, o in zip(range(batch_size), left_child.split(1), right_child.split(1),
   402       139    1137000.0   8179.9      0.0                                                 node_stacks, target[t].tolist(), embeddings_stacks):
   403      1168    1555000.0   1331.3      0.0              current_token = output_lang.ids_to_tokens([i])
   404      1168   20387000.0  17454.6      0.2              current_equation = output_lang.ids_to_tokens(target.transpose(0,1)[idx])
   405                                                       #print("at token", current_token, "in", current_equation)
   406                                                       #print("current node_stack length", len(node_stack))
   407                                                       # for 
   408                                                       #   batch_num
   409                                                       #   the left child: h_l 
   410                                                       #   the right child: h_r
   411      1168     392000.0    335.6      0.0              if len(node_stack) != 0:
   412       730    1316000.0   1802.7      0.0                  node = node_stack.pop()
   413                                                           #print("removed last from node_stack, now", len(node_stack), "elems")
   414                                                       else:
   415       438     106000.0    242.0      0.0                  left_childs.append(None)
   416       438      57000.0    130.1      0.0                  continue
   417                                           
   418                                                       # i is the num in language of where that specific language token is
   419                                                       # if i is an operator
   420       730     148000.0    202.7      0.0              if i < num_start:
   421                                                           #print(current_token, "is an operator, making a left and right node")
   422                                                           # make a left and right tree node
   423       305     467000.0   1531.1      0.0                  node_stack.append(TreeNode(r))
   424       305     318000.0   1042.6      0.0                  node_stack.append(TreeNode(l, left_flag=True))
   425                                                           # save the embedding of the operator 
   426                                                           # terminal means a leaf node
   427       305    1488000.0   4878.7      0.0                  o.append(TreeEmbedding(node_label[idx].unsqueeze(0), False))
   428                                                           #print("saving node embedding to o (non terminal node), and r, and l to node_stack. o now of size", len(o), "node_stack of size", len(node_stack))
   429                                                       else:
   430                                                           #print(current_token, "is not an operator")
   431                                                           # otherwise its either a number from the input equation or a copy number
   432                                                           # we have a list (o) of the current nodes in the tree
   433                                                           # if we have a leaf node at the top of the stack, get it.
   434                                                           # next element in the stack must be an operator, so get it 
   435                                                           # and combine the new node, operator, and other element
   436                                           
   437                                                           # current_nums_embedding: batch_size x num_length x hidden_size
   438                                                           # current_num = num_embedding of the number selected
   439       425    2673000.0   6289.4      0.0                  current_num = current_nums_embeddings[idx, i - num_start].unsqueeze(0)
   440                                                           # while there are tokens in the embedding stack and the last element IS a leaf node
   441       730     575000.0    787.7      0.0                  while len(o) > 0 and o[-1].terminal:
   442                                                               #print("terminal element in o, getting terminal element and operator, and merging")
   443                                                               # get the two elements from it
   444       305    7596000.0  24904.9      0.1                      sub_stree = o.pop()
   445       305     345000.0   1131.1      0.0                      op = o.pop()
   446                                                               # contains equation (13)
   447                                                               # this combines a left and right tree along with a node
   448       305   64664000.0 212013.1      0.8                      current_num = merge(op.embedding, sub_stree.embedding, current_num)
   449                                                               #print('merged. o now of size', len(o))
   450                                                           # then re-add the node back to the stack
   451                                                           #print("adding current_num to o (terminal node)")
   452       425     605000.0   1423.5      0.0                  o.append(TreeEmbedding(current_num, True))
   453       730     319000.0    437.0      0.0              if len(o) > 0 and o[-1].terminal:
   454                                                           #print("terminal element in o, adding to left child")
   455                                                           # left_childs is a running vector of the sub tree embeddings "t" 
   456                                                           # need this for generation of the right q
   457       425     160000.0    376.5      0.0                  left_childs.append(o[-1].embedding)
   458                                                       else:
   459       305      95000.0    311.5      0.0                  left_childs.append(None)
   460                                           
   461                                               # all_leafs = torch.stack(all_leafs, dim=1)  # B x S x 2
   462                                               
   463                                               # all_node_outputs:  for each token in the equation:
   464                                               #   the current scoring of nums for each batch
   465                                               # 
   466                                               # transform to 
   467                                               # all_node_outputs2: for each batch:
   468                                               #   the current scoring of nums for each token in equation
   469                                               # = batch_size x max_len x num_nums
   470        15     233000.0  15533.3      0.0      all_node_outputs2 = torch.stack(all_node_outputs, dim=1)  # B x S x N
   471                                           
   472        15      59000.0   3933.3      0.0      target = target.transpose(0, 1).contiguous()
   473        15      11000.0    733.3      0.0      if USE_CUDA:
   474                                                   # all_leafs = all_leafs.cuda()
   475                                                   all_node_outputs2 = all_node_outputs2.cuda()
   476                                                   target = target.cuda()
   477                                           
   478                                               # for batch in target:
   479                                               #     print([output_lang.index2word[_] for _ in batch])
   480                                               #print('done equation')
   481        15    6307000.0 420466.7      0.1      loss = masked_cross_entropy(all_node_outputs2, target, target_length)
   482        15 5088017000.0    3e+08     62.1      loss.backward()
   483                                           
   484                                               # Update parameters with optimizers
   485        15  248646000.0    2e+07      3.0      encoder_optimizer.step()
   486        15  111349000.0    7e+06      1.4      predict_optimizer.step()
   487        15   89984000.0    6e+06      1.1      generate_optimizer.step()
   488        15   41128000.0    3e+06      0.5      merge_optimizer.step()
   489        15      24000.0   1600.0      0.0      return loss.item() 

  2.76 seconds - /Users/home/school/thesis/TESTING/math_seq2tree/src/models.py:268 - forward
  5.27 seconds - /Users/home/school/thesis/TESTING/math_seq2tree/src/train_and_evaluate.py:491 - evaluate_tree
  8.20 seconds - /Users/home/school/thesis/TESTING/math_seq2tree/src/train_and_evaluate.py:257 - train_tree
