Timer unit: 1e-09 s

Total time: 31.1438 s
File: /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py
Function: train_tree at line 259

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   259                                           @line_profiler.profile
   260                                           def train_tree(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, output_var_batches, generate_nums, models, output_lang, num_pos, useCustom, all_vars, english=False):
   261                                               # input_batch: padded inputs
   262                                               # input_length: length of the inputs (without padding)
   263                                               # target_batch: padded outputs
   264                                               # target_length: length of the outputs (without padding)
   265                                               # num_stack_batch: the corresponding nums lists
   266                                               # num_size_batch: number of numbers from the input text
   267                                               # generate_nums: numbers to generate
   268                                               # num_pos: positions of the numbers lists
   269                                           
   270                                           
   271                                           
   272                                               # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)
   273        25    3240000.0 129600.0      0.0      input_var = torch.LongTensor(input_batch).transpose(0, 1)
   274        25     167000.0   6680.0      0.0      problem_vars = torch.LongTensor(output_var_batches)
   275        25     508000.0  20320.0      0.0      target = torch.LongTensor(target_batch)#.transpose(0, 1)
   276                                           
   277                                               # num vars total in the output lang. will need to mask ones not in the current equation
   278                                              # num_total_vars = len(problem_vars[0])
   279        25     154000.0   6160.0      0.0      num_equations_per_obs = torch.LongTensor([len(equ_set) for equ_set in target_batch])
   280        25    4215000.0 168600.0      0.0      num_total_vars = max(num_equations_per_obs)
   281                                           
   282                                               # sequence mask for attention
   283                                               # 0s where in input, 1s where not in input
   284        25       8000.0    320.0      0.0      seq_mask = []
   285        25      17000.0    680.0      0.0      max_len = max(input_length)
   286       271      51000.0    188.2      0.0      for i in input_length:
   287       246     761000.0   3093.5      0.0          seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])
   288        25     448000.0  17920.0      0.0      seq_mask = torch.ByteTensor(seq_mask)
   289                                           
   290                                               # number mask 
   291                                               # 0s where the numbers are from input, 1s where not in input
   292        25       1000.0     40.0      0.0      num_mask = []
   293        25       8000.0    320.0      0.0      if useCustom:
   294        25      19000.0    760.0      0.0          max_num_size = max(num_size_batch) + len(generate_nums) + len(all_vars) 
   295                                               else:
   296                                                   max_num_size = max(num_size_batch) + len(generate_nums) 
   297                                               # in language its 
   298                                               # operators + gen numbers + vars + copy numbers
   299       271      81000.0    298.9      0.0      for i, num_size in enumerate(num_size_batch):
   300       246      18000.0     73.2      0.0          if useCustom:
   301       246    1239000.0   5036.6      0.0              d = num_size + len(problem_vars[i].tolist()) + len(generate_nums)
   302       246     468000.0   1902.4      0.0              num_mask.append([0] * len(generate_nums) + problem_vars[i].tolist() + [0] * num_size + [1] * (max_num_size - d))
   303                                                   else:
   304                                                       d = num_size + len(generate_nums)
   305                                                       num_mask.append([0] * len(generate_nums) + [0] * num_size + [1] * (max_num_size - d))
   306        25     292000.0  11680.0      0.0      num_mask = torch.ByteTensor(num_mask)
   307                                           
   308        25      32000.0   1280.0      0.0      unk = output_lang.word2index["UNK"]
   309        25    1981000.0  79240.0      0.0      padding_hidden = torch.FloatTensor([0.0 for _ in range(models['predict'].hidden_size)]).unsqueeze(0)
   310                                           
   311                                           
   312        25       9000.0    360.0      0.0      if USE_CUDA:
   313                                                   input_var = input_var.cuda()
   314                                                   seq_mask = seq_mask.cuda()
   315                                                   padding_hidden = padding_hidden.cuda()
   316                                                   num_mask = num_mask.cuda()
   317        25       2000.0     80.0      0.0      batch_size = len(input_length)
   318                                           
   319        25       7000.0    280.0      0.0      total_loss = None
   320        25       2000.0     80.0      0.0      total_acc = [] 
   321                                           
   322                                               # Run words through encoder
   323                                               # embedding + dropout layer
   324                                               # encoder_outputs: num_batches x 512 q_0 vector
   325                                               # problem_output: max_length x num_batches x hidden_size
   326        25  952296000.0    4e+07      3.1      encoder_outputs, problem_output = models['encoder'](input_var, input_length)
   327                                               # Prepare input and output variables
   328                                           
   329                                               
   330                                               # array of len of numbers that must be copied from the input text
   331        25     145000.0   5800.0      0.0      copy_num_len = [len(_) for _ in num_pos]
   332                                               # max nums to copy
   333        25      48000.0   1920.0      0.0      num_size = max(copy_num_len)
   334                                           
   335                                               # num_batches x num_size x hidden_size that correspond to the embeddings of the numbers
   336        50   32737000.0 654740.0      0.1      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size,
   337        25      18000.0    720.0      0.0                                                              models['encoder'].hidden_size)
   338                                           
   339                                           
   340                                               # index in the language where the special (operators) tokens end and input/output text begins
   341        25      24000.0    960.0      0.0      num_start = output_lang.num_start
   342                                           
   343        25 5028009000.0    2e+08     16.1      pred_num_equations = models['num_x_predict'](encoder_outputs)
   344                                           
   345        25       9000.0    360.0      0.0      if useCustom:
   346                                                   # node that max(num_equations_per_obs) should be the same as the lenth of vars
   347        25   51441000.0    2e+06      0.2          xs = models['x_generate'](len(all_vars), encoder_outputs, problem_output)
   348                                               else: 
   349                                                   xs = None 
   350                                           
   351        25      13000.0    520.0      0.0      if useCustom:
   352        25  109957000.0    4e+06      0.4          qs = models['x_to_q'](encoder_outputs, xs)
   353                                               else:
   354                                                   qs = problem_output
   355                                           
   356                                               # do equations one at a time
   357        84    1053000.0  12535.7      0.0      for cur_equation in range(max(num_equations_per_obs)):
   358                                                   # select the ith equation in each obs
   359        59    8084000.0 137016.9      0.0          ith_equation_target = deepcopy(target[:, cur_equation, :].transpose(0,1))
   360        59      12000.0    203.4      0.0          if useCustom:
   361        59     484000.0   8203.4      0.0              ith_equation_goal = qs[:, cur_equation, :]
   362        59    2764000.0  46847.5      0.0              node_stacks = [[TreeNode(_)] for _ in ith_equation_goal.split(1, dim=0)]
   363                                                   else:
   364                                                       ith_equation_goal = problem_output
   365                                                       node_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]
   366                                           
   367        59     974000.0  16508.5      0.0          ith_equation_target_lengths = torch.Tensor(target_length)[:, cur_equation]
   368        59      36000.0    610.2      0.0          ith_equation_num_stacks = []
   369       641     122000.0    190.3      0.0          for stack in nums_stack_batch:
   370       582     162000.0    278.4      0.0              ith_equation_num_stacks.append(stack[cur_equation])
   371                                           
   372                                                   # max_target_length = int(max(ith_equation_target_lengths.tolist()))
   373        59     157000.0   2661.0      0.0          max_target_length = len(ith_equation_target)
   374                                           
   375        59     421000.0   7135.6      0.0          all_node_outputs = []
   376        59    1666000.0  28237.3      0.0          embeddings_stacks = [[] for _ in range(batch_size)]
   377        59     231000.0   3915.3      0.0          left_childs = [None for _ in range(batch_size)]
   378                                           
   379      1085     319000.0    294.0      0.0          for t in range(max_target_length):
   380                                           
   381                                                       # predict gets the encodings and embeddings for the current node 
   382                                                       #   num_score: batch_size x num_length
   383                                                       #       likliehood prediction of each number
   384                                                       #   op: batch_size x num_ops
   385                                                       #       likliehood of the operator tokens
   386                                                       #   current_embeddings: batch_size x 1 x hidden_size
   387                                                       #       goal vector (q) for the current node 
   388                                                       #   current_context: batch_size x 1 x hidden_size
   389                                                       #       context vector (c) for the subtree
   390                                                       #   embedding_weight: batch_size x num_length x hidden_size
   391                                                       #       embeddings of the generate and copy numbers
   392                                           
   393      2052 4768435000.0    2e+06     15.3              num_score, op, current_embeddings, current_context, current_nums_embeddings = models['predict'](
   394      1026     336000.0    327.5      0.0                  node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, xs, seq_mask, num_mask, useCustom)
   395                                           
   396                                           
   397                                                       # this is mainly what we want to train
   398      1026    7487000.0   7297.3      0.0              outputs = torch.cat((op, num_score), 1)
   399      1026     795000.0    774.9      0.0              all_node_outputs.append(outputs)
   400                                           
   401                                                       # target[t] is the equation character at index t for each batch
   402                                                       #    target[t] = 1 x num_batches
   403                                                       # outputs is the strength of operators or a number token
   404                                                       # num_stack_batch is the cooresponding num lists
   405                                                       # num_start is where non-operators begin
   406                                                       # unk is unknown token
   407                                                       # returns
   408                                                       #   for position t in each equation
   409                                                       #       target_t: actual equation value
   410                                                       #       generate_input: equation value if its an operator
   411      1026   51682000.0  50372.3      0.2              target_t, generate_input = generate_tree_input(ith_equation_target[t].tolist(), outputs, ith_equation_num_stacks, num_start, unk)
   412      1026    5120000.0   4990.3      0.0              ith_equation_target[t] = target_t
   413      1026     444000.0    432.7      0.0              if USE_CUDA:
   414                                                           generate_input = generate_input.cuda()
   415                                           
   416                                                       # takes:
   417                                                       #     generate a left and right child node with a label
   418                                                       #     current_embeddings: q : batch_size x 1 x hidden_dim
   419                                                       #     generate_input: [operator tokens at position t]
   420                                                       #     current_context: c : batch_size x 1 x hidden_dim
   421                                                       # returns
   422                                                       #     l_child: batch_size x hidden_dim
   423                                                       #          hidden state h_l:
   424                                                       #     r_child: batch_size x hidden_dim
   425                                                       #          hidden state h_r:
   426                                                       #     node_label_ : batch_size x embedding_size 
   427                                                       #          basically the context vector (c)
   428                                                       # the node generation takes the first half of equations (10) and (11) 
   429      1026 1193158000.0    1e+06      3.8              left_child, right_child, node_label = models['generate'](current_embeddings, generate_input, current_context)
   430      1026     668000.0    651.1      0.0              left_childs = []
   431     12128   90058000.0   7425.6      0.3              for idx, l, r, node_stack, i, o in zip(range(batch_size), left_child.split(1), right_child.split(1),
   432      1026    7166000.0   6984.4      0.0                                                  node_stacks, ith_equation_target[t].tolist(), embeddings_stacks):
   433                                                           # current_token = output_lang.ids_to_tokens([i])
   434                                                           # current_equation = output_lang.ids_to_tokens(target.transpose(0,1)[idx])
   435                                                           #print("at token", current_token, "in", current_equation)
   436                                                           #print("current node_stack length", len(node_stack))
   437                                                           # for 
   438                                                           #   batch_num
   439                                                           #   the left child: h_l 
   440                                                           #   the right child: h_r
   441     10076    3041000.0    301.8      0.0                  if len(node_stack) != 0:
   442      6593    8360000.0   1268.0      0.0                      node = node_stack.pop()
   443                                                               #print("removed last from node_stack, now", len(node_stack), "elems")
   444                                                           else:
   445      3483     868000.0    249.2      0.0                      left_childs.append(None)
   446      3483     376000.0    108.0      0.0                      continue
   447                                           
   448                                                           # i is the num in language of where that specific language token is
   449                                                           # if i is an operator
   450      6593    1270000.0    192.6      0.0                  if i < num_start:
   451                                                               #print(current_token, "is an operator, making a left and right node")
   452                                                               # make a left and right tree node
   453      5272    4084000.0    774.7      0.0                      node_stack.append(TreeNode(r))
   454      5272    3802000.0    721.2      0.0                      node_stack.append(TreeNode(l, left_flag=True))
   455                                                               # save the embedding of the operator 
   456                                                               # terminal means a leaf node
   457      5272   21071000.0   3996.8      0.1                      o.append(TreeEmbedding(node_label[idx].unsqueeze(0), False))
   458                                                               #print("saving node embedding to o (non terminal node), and r, and l to node_stack. o now of size", len(o), "node_stack of size", len(node_stack))
   459                                                           else:
   460                                                               #print(current_token, "is not an operator")
   461                                                               # otherwise its either a number from the input equation or a copy number
   462                                                               # we have a list (o) of the current nodes in the tree
   463                                                               # if we have a leaf node at the top of the stack, get it.
   464                                                               # next element in the stack must be an operator, so get it 
   465                                                               # and combine the new node, operator, and other element
   466                                           
   467                                                               # current_nums_embedding: batch_size x num_length x hidden_size
   468                                                               # current_num = num_embedding of the number selected
   469      1321    9298000.0   7038.6      0.0                      current_num = current_nums_embeddings[idx, i - num_start].unsqueeze(0)
   470                                                               # while there are tokens in the embedding stack and the last element IS a leaf node
   471      2286    1645000.0    719.6      0.0                      while len(o) > 0 and o[-1].terminal:
   472                                                                   #print("terminal element in o, getting terminal element and operator, and merging")
   473                                                                   # get the two elements from it
   474       965   22541000.0  23358.5      0.1                          sub_stree = o.pop()
   475       965     858000.0    889.1      0.0                          op = o.pop()
   476                                                                   # contains equation (13)
   477                                                                   # this combines a left and right tree along with a node
   478       965  183131000.0 189773.1      0.6                          current_num = models['merge'](op.embedding, sub_stree.embedding, current_num)
   479                                                                   #print('merged. o now of size', len(o))
   480                                                               # then re-add the node back to the stack
   481                                                               #print("adding current_num to o (terminal node)")
   482      1321    1716000.0   1299.0      0.0                      o.append(TreeEmbedding(current_num, True))
   483      6593    2395000.0    363.3      0.0                  if len(o) > 0 and o[-1].terminal:
   484                                                               #print("terminal element in o, adding to left child")
   485                                                               # left_childs is a running vector of the sub tree embeddings "t" 
   486                                                               # need this for generation of the right q
   487      1321     488000.0    369.4      0.0                      left_childs.append(o[-1].embedding)
   488                                                           else:
   489      5272    1235000.0    234.3      0.0                      left_childs.append(None)
   490                                           
   491                                                   # all_leafs = torch.stack(all_leafs, dim=1)  # B x S x 2
   492                                                   
   493                                                   # all_node_outputs:  for each token in the equation:
   494                                                   #   the current scoring of nums for each batch
   495                                                   # 
   496                                                   # transform to 
   497                                                   # all_node_outputs2: for each batch:
   498                                                   #   the current scoring of nums for each token in equation
   499                                                   # = batch_size x max_len x num_nums
   500        59    1091000.0  18491.5      0.0          all_node_outputs2 = torch.stack(all_node_outputs, dim=1)  # B x S x N
   501                                           
   502        59     758000.0  12847.5      0.0          ith_equation_target = ith_equation_target.transpose(0, 1).contiguous()
   503        59      20000.0    339.0      0.0          if USE_CUDA:
   504                                                       # all_leafs = all_leafs.cuda()
   505                                                       all_node_outputs2 = all_node_outputs2.cuda()
   506                                                       ith_equation_target = ith_equation_target.cuda()
   507                                           
   508                                                   # for batch in target:
   509                                                   #     print([output_lang.index2word[_] for _ in batch])
   510                                                   #print('done equation')
   511                                                   # loss = masked_cross_entropy(all_node_outputs2, target, target_length)
   512        59   14930000.0 253050.8      0.0          current_equation_loss = torch.nn.CrossEntropyLoss(reduction="none")(all_node_outputs2.view(-1, all_node_outputs2.size(2)), ith_equation_target.view(-1).to(device)).mean()
   513        59      18000.0    305.1      0.0          same = 0
   514        59      12000.0    203.4      0.0          lengths = 0
   515        59    1395000.0  23644.1      0.0          print(f'Equation {cur_equation}')
   516       641    1424000.0   2221.5      0.0          for i, batch in enumerate(all_node_outputs2):
   517       582     988000.0   1697.6      0.0              vals = []
   518       582    1624000.0   2790.4      0.0              equ_length = int(ith_equation_target_lengths[i].item())
   519      3457    8628000.0   2495.8      0.0              for j, probs in enumerate(batch):
   520      3424    7798000.0   2277.5      0.0                  max_val = torch.argmax(probs)
   521      3424     828000.0    241.8      0.0                  vals.append(max_val)
   522      3424     529000.0    154.5      0.0                  lengths += 1
   523      3424     464000.0    135.5      0.0                  if j > equ_length:
   524       549    1568000.0   2856.1      0.0                      break
   525      2875   11251000.0   3913.4      0.0                  if max_val == ith_equation_target[i][j]:
   526       966     175000.0    181.2      0.0                      same += 1
   527       582    3701000.0   6359.1      0.0              print(f"        prediction: {[output_lang.index2word[_] for _ in vals[0:equ_length]]}")
   528       582    6498000.0  11164.9      0.0              print(f"        actual: {[output_lang.index2word[_] for _ in ith_equation_target[i][0:equ_length]]}")
   529        59    4108000.0  69627.1      0.0          if total_loss != None:
   530        34     387000.0  11382.4      0.0              total_loss += current_equation_loss
   531                                                   else:
   532        25       6000.0    240.0      0.0              total_loss = current_equation_loss
   533        59      85000.0   1440.7      0.0          total_acc += [same/lengths]
   534                                               
   535                                               # add the loss of number equations
   536        25    1029000.0  41160.0      0.0      num_x_loss = torch.nn.CrossEntropyLoss()(pred_num_equations, num_equations_per_obs.to(device))
   537                                                       
   538        25      71000.0   2840.0      0.0      total_loss += num_x_loss
   539        25        2e+10    7e+08     59.3      total_loss.backward()
   540                                           
   541                                               # Update parameters with optimizers
   542        25     361000.0  14440.0      0.0      return total_loss.item(), sum(total_acc)/len(total_acc)

Total time: 39.655 s
File: /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py
Function: evaluate_tree at line 544

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   544                                           @line_profiler.profile
   545                                           def evaluate_tree(input_batch, input_length, generate_nums, models, output_lang, num_pos,
   546                                                             vars, useCustom, beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):
   547                                           
   548        37     936000.0  25297.3      0.0      seq_mask = torch.ByteTensor(1, input_length).fill_(0)
   549                                               # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)
   550        37     662000.0  17891.9      0.0      input_var = torch.LongTensor(input_batch).unsqueeze(1)
   551                                           
   552                                           
   553                                               # Set to not-training mode to disable dropout
   554                                           
   555        37    2086000.0  56378.4      0.0      padding_hidden = torch.FloatTensor([0.0 for _ in range(models['predict'].hidden_size)]).unsqueeze(0)
   556                                           
   557        37       7000.0    189.2      0.0      batch_size = 1
   558                                           
   559        37      25000.0    675.7      0.0      if USE_CUDA:
   560                                                   input_var = input_var.cuda()
   561                                                   seq_mask = seq_mask.cuda()
   562                                                   padding_hidden = padding_hidden.cuda()
   563                                                   # num_mask = num_mask.cuda()
   564                                           
   565                                               # Run words through encoder
   566        37  249637000.0    7e+06      0.6      encoder_outputs, problem_output = models['encoder'](input_var, [input_length])
   567                                           
   568                                               # Prepare input and output variables
   569        37     718000.0  19405.4      0.0      node_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]
   570                                           
   571        37      29000.0    783.8      0.0      num_size = len(num_pos)
   572        74    7735000.0 104527.0      0.0      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,
   573        37      17000.0    459.5      0.0                                                                models['encoder'].hidden_size)
   574        37      36000.0    973.0      0.0      num_start = output_lang.num_start
   575                                               # B x P x N
   576                                           
   577                                               # predict number of xs
   578        37 1242867000.0    3e+07      3.1      num_x = models['num_x_predict'](encoder_outputs, eval=True).argmax().item()
   579                                           
   580                                               # if useCustom:
   581                                               #     # make random x vectors
   582                                               #     x_list = []
   583                                               #     x_list.append(torch.rand(2, 512))
   584                                               #     # # fill other as zero
   585                                               #     # x_list.append(torch.zeros(512))
   586                                               #     x_list = torch.stack(x_list)
   587                                               # else:
   588                                               #     x_list = None
   589                                           
   590                                               # get xs
   591        37      16000.0    432.4      0.0      if useCustom:
   592        37   11505000.0 310945.9      0.0          xs = models['x_generate'](num_x, encoder_outputs, problem_output)
   593                                               else: 
   594                                                   xs = None
   595                                           
   596                                               # padd the xs in the first dim to match the length of variables
   597        37     435000.0  11756.8      0.0      padding = torch.zeros(1, len(vars) - num_x, 512)
   598        37     306000.0   8270.3      0.0      xs = torch.cat((xs, padding.to(device)), dim=1).to(device)
   599                                               # get qs
   600        37      16000.0    432.4      0.0      if useCustom:
   601        37   18010000.0 486756.8      0.0          qs = models['x_to_q'](encoder_outputs, xs)
   602                                               else:
   603                                                   qs = problem_output
   604                                           
   605                                           
   606                                               # num_mask = torch.ByteTensor(1, len(num_pos) + len(generate_nums)).fill_(0)
   607        37      10000.0    270.3      0.0      num_mask = []
   608                                               # max_num_size = num_pos + len(generate_nums) + len(vars) 
   609                                               # in language its 
   610                                               # operators + gen numbers + vars + copy numbers
   611        37      10000.0    270.3      0.0      if useCustom:
   612        37      93000.0   2513.5      0.0          num_mask.append([0] * num_size + [0] * num_x + [1] * (len(vars) - num_x) + [0] * len(generate_nums))
   613                                               else:
   614                                                   num_mask.append([0] * num_size +  [0] * len(generate_nums))
   615        37     479000.0  12945.9      0.0      num_mask = torch.ByteTensor(num_mask).to(device)
   616                                           
   617                                           
   618        37       7000.0    189.2      0.0      final_beams = []
   619                                           
   620       110      86000.0    781.8      0.0      for i in range(max(num_x, 1)):
   621                                                   # get the node stacks
   622        74       4000.0     54.1      0.0          if useCustom:
   623        74     831000.0  11229.7      0.0              ith_equation_goal = qs[:, i, :]
   624        74    1299000.0  17554.1      0.0              node_stacks = [[TreeNode(_)] for _ in ith_equation_goal.split(1, dim=0)]
   625                                                   else:
   626                                                       ith_equation_goal = problem_output
   627                                                       node_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]
   628                                           
   629        74     174000.0   2351.4      0.0          embeddings_stacks = [[] for _ in range(batch_size)]
   630        74      61000.0    824.3      0.0          left_childs = [None for _ in range(batch_size)]
   631                                           
   632                                                   # evaulation uses beam search
   633                                                   # key is how the beams are compared
   634        74     834000.0  11270.3      0.0          beams = [TreeBeam(0.0, node_stacks, embeddings_stacks, left_childs, [])]
   635                                           
   636      3379    1513000.0    447.8      0.0          for t in range(max_length):
   637      3306  770309000.0 233003.3      1.9              current_beams = []
   638     19539    9960000.0    509.7      0.0              while len(beams) > 0:
   639     16234   30969000.0   1907.7      0.1                  b = beams.pop()
   640     16234    7676000.0    472.8      0.0                  if len(b.node_stack[0]) == 0:
   641       146      29000.0    198.6      0.0                      current_beams.append(b)
   642       146      16000.0    109.6      0.0                      continue
   643                                                           # left_childs = torch.stack(b.left_childs)
   644     16088    3580000.0    222.5      0.0                  left_childs = b.left_childs
   645                                           
   646     16088        1e+10 877957.8     35.6                  num_score, op, current_embeddings, current_context, current_nums_embeddings = models['predict']( b.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, xs, seq_mask, num_mask, useCustom)
   647                                           
   648                                                           # leaf = p_leaf[:, 0].unsqueeze(1)
   649                                                           # repeat_dims = [1] * leaf.dim()
   650                                                           # repeat_dims[1] = op.size(1)
   651                                                           # leaf = leaf.repeat(*repeat_dims)
   652                                                           #
   653                                                           # non_leaf = p_leaf[:, 1].unsqueeze(1)
   654                                                           # repeat_dims = [1] * non_leaf.dim()
   655                                                           # repeat_dims[1] = num_score.size(1)
   656                                                           # non_leaf = non_leaf.repeat(*repeat_dims)
   657                                                           #
   658                                                           # p_leaf = torch.cat((leaf, non_leaf), dim=1)
   659     16087  201474000.0  12524.0      0.5                  out_score = nn.functional.log_softmax(torch.cat((op, num_score), dim=1), dim=1)
   660                                           
   661                                                           # out_score = p_leaf * out_score
   662                                           
   663                                                           # topv:
   664                                                           #   largest elements in the out_score
   665                                                           # topi:
   666                                                           #   indexes of the largest elements 
   667     16087  123386000.0   7669.9      0.3                  topv, topi = out_score.topk(beam_size)
   668                                           
   669                                                           # is_leaf = int(topi[0])
   670                                                           # if is_leaf:
   671                                                           #     topv, topi = op.topk(1)
   672                                                           #     out_token = int(topi[0])
   673                                                           # else:
   674                                                           #     topv, topi = num_score.topk(1)
   675                                                           #     out_token = int(topi[0]) + num_start
   676                                           
   677                                                           # for the largest element, and its index
   678     96522  712219000.0   7378.8      1.8                  for tv, ti in zip(topv.split(1, dim=1), topi.split(1, dim=1)):
   679     80435  572342000.0   7115.6      1.4                      current_node_stack = copy_list(b.node_stack)
   680     80435   11103000.0    138.0      0.0                      current_left_childs = []
   681     80435  501675000.0   6237.0      1.3                      current_embeddings_stacks = copy_list(b.embedding_stack)
   682     80435 1727911000.0  21482.1      4.4                      current_out = copy.deepcopy(b.out)
   683                                           
   684                                                               # the predicted token is that of the highest score relation
   685     80435   77112000.0    958.7      0.2                      out_token = int(ti)
   686                                                               # save token 
   687     80435   17075000.0    212.3      0.0                      current_out.append(out_token)
   688                                           
   689     80435   27685000.0    344.2      0.1                      node = current_node_stack[0].pop()
   690                                           
   691                                                               # if the predicted token is an operator
   692     80435   11892000.0    147.8      0.0                      if out_token < num_start:
   693                                                                   # this is the token to generate l and r from
   694     48235  247959000.0   5140.6      0.6                          generate_input = torch.LongTensor([out_token])
   695     48235   10772000.0    223.3      0.0                          if USE_CUDA:
   696                                                                       generate_input = generate_input.cuda()
   697                                                                   # get the left and right children and current label
   698     48235        1e+10 307830.4     37.4                          left_child, right_child, node_label = models['generate'](current_embeddings, generate_input, current_context)
   699                                           
   700     48235   79981000.0   1658.2      0.2                          current_node_stack[0].append(TreeNode(right_child))
   701     48235   43443000.0    900.7      0.1                          current_node_stack[0].append(TreeNode(left_child, left_flag=True))
   702                                           
   703     48235  340398000.0   7057.1      0.9                          current_embeddings_stacks[0].append(TreeEmbedding(node_label[0].unsqueeze(0), False))
   704                                                               else:
   705                                                                   # predicted token is a number
   706                                                                   # get the token embedding - embedding of either the generate num or copy num
   707     32200  184473000.0   5729.0      0.5                          current_num = current_nums_embeddings[0, out_token - num_start].unsqueeze(0)
   708                                                                   
   709                                                                   # if we are a right node (there is a left node and operator)
   710     32264   14502000.0    449.5      0.0                          while len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:
   711        64      19000.0    296.9      0.0                              sub_stree = current_embeddings_stacks[0].pop()
   712        64      20000.0    312.5      0.0                              op = current_embeddings_stacks[0].pop()
   713        64    9892000.0 154562.5      0.0                              current_num = models['merge'](op.embedding, sub_stree.embedding, current_num)
   714                                                                   # save node (or subtree) to the embeddings list
   715     32200   17913000.0    556.3      0.0                          current_embeddings_stacks[0].append(TreeEmbedding(current_num, True))
   716     80435   43961000.0    546.5      0.1                      if len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:
   717     32200    8939000.0    277.6      0.0                          current_left_childs.append(current_embeddings_stacks[0][-1].embedding)
   718                                                               else:
   719     48235   15729000.0    326.1      0.0                          current_left_childs.append(None)
   720                                                               # the beam "score" is the sum of the associations 
   721    160870 3255762000.0  20238.5      8.2                      current_beams.append(TreeBeam(b.score+float(tv), current_node_stack, current_embeddings_stacks,
   722     80435   14947000.0    185.8      0.0                                                  current_left_childs, current_out))
   723                                                       # order beam by highest to lowest
   724      3305   30413000.0   9202.1      0.1              beams = sorted(current_beams, key=lambda x: x.score, reverse=True)
   725      3305    2350000.0    711.0      0.0              beams = beams[:beam_size]
   726      3305     458000.0    138.6      0.0              flag = True
   727     19830    5271000.0    265.8      0.0              for b in beams:
   728     16525    5211000.0    315.3      0.0                  if len(b.node_stack[0]) != 0:
   729     16379    1776000.0    108.4      0.0                      flag = False
   730      3305    1074000.0    325.0      0.0              if flag:
   731                                                           break
   732                                           
   733        73      47000.0    643.8      0.0          final_beams.append(beams[0].out)
   734        36       6000.0    166.7      0.0      return final_beams

 31.14 seconds - /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py:259 - train_tree
 39.65 seconds - /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py:544 - evaluate_tree
