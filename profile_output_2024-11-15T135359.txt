Timer unit: 1e-09 s

Total time: 14.0996 s
File: /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py
Function: train_tree at line 259

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   259                                           @line_profiler.profile
   260                                           def train_tree(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, output_var_batches, generate_nums, models, output_lang, num_pos, useCustom, all_vars, english=False):
   261                                               # input_batch: padded inputs
   262                                               # input_length: length of the inputs (without padding)
   263                                               # target_batch: padded outputs
   264                                               # target_length: length of the outputs (without padding)
   265                                               # num_stack_batch: the corresponding nums lists
   266                                               # num_size_batch: number of numbers from the input text
   267                                               # generate_nums: numbers to generate
   268                                               # num_pos: positions of the numbers lists
   269                                           
   270                                           
   271                                           
   272                                               # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)
   273         9     811000.0  90111.1      0.0      input_var = torch.LongTensor(input_batch).transpose(0, 1)
   274         9      76000.0   8444.4      0.0      problem_vars = torch.LongTensor(output_var_batches)
   275         9     127000.0  14111.1      0.0      target = torch.LongTensor(target_batch)#.transpose(0, 1)
   276                                           
   277                                               # num vars total in the output lang. will need to mask ones not in the current equation
   278                                              # num_total_vars = len(problem_vars[0])
   279         9     103000.0  11444.4      0.0      num_equations_per_obs = torch.LongTensor([len(equ_set) for equ_set in target_batch])
   280         9     663000.0  73666.7      0.0      num_total_vars = max(num_equations_per_obs)
   281                                           
   282                                               # sequence mask for attention
   283                                               # 0s where in input, 1s where not in input
   284         9       6000.0    666.7      0.0      seq_mask = []
   285         9      13000.0   1444.4      0.0      max_len = max(input_length)
   286        84      25000.0    297.6      0.0      for i in input_length:
   287        75     603000.0   8040.0      0.0          seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])
   288         9     539000.0  59888.9      0.0      seq_mask = torch.ByteTensor(seq_mask)
   289                                           
   290                                               # number mask 
   291                                               # 0s where the numbers are from input, 1s where not in input
   292         9       4000.0    444.4      0.0      num_mask = []
   293         9       5000.0    555.6      0.0      if useCustom:
   294                                                   max_num_size = max(num_size_batch) + len(generate_nums) + len(all_vars) 
   295                                               else:
   296         9      33000.0   3666.7      0.0          max_num_size = max(num_size_batch) + len(generate_nums) 
   297                                               # in language its 
   298                                               # operators + gen numbers + vars + copy numbers
   299        84     129000.0   1535.7      0.0      for i, num_size in enumerate(num_size_batch):
   300        75      30000.0    400.0      0.0          if useCustom:
   301                                                       d = num_size + len(problem_vars[i].tolist()) + len(generate_nums)
   302                                                       num_mask.append([0] * len(generate_nums) + problem_vars[i].tolist() + [0] * num_size + [1] * (max_num_size - d))
   303                                                   else:
   304        75      41000.0    546.7      0.0              d = num_size + len(generate_nums)
   305        75     151000.0   2013.3      0.0              num_mask.append([0] * len(generate_nums) + [0] * num_size + [1] * (max_num_size - d))
   306         9     162000.0  18000.0      0.0      num_mask = torch.ByteTensor(num_mask)
   307                                           
   308         9      30000.0   3333.3      0.0      unk = output_lang.word2index["UNK"]
   309         9    1332000.0 148000.0      0.0      padding_hidden = torch.FloatTensor([0.0 for _ in range(models['predict'].hidden_size)]).unsqueeze(0)
   310                                           
   311                                           
   312         9       8000.0    888.9      0.0      if USE_CUDA:
   313                                                   input_var = input_var.cuda()
   314                                                   seq_mask = seq_mask.cuda()
   315                                                   padding_hidden = padding_hidden.cuda()
   316                                                   num_mask = num_mask.cuda()
   317         9       6000.0    666.7      0.0      batch_size = len(input_length)
   318                                           
   319         9       6000.0    666.7      0.0      total_loss = None
   320         9       1000.0    111.1      0.0      total_acc = [] 
   321                                           
   322                                               # Run words through encoder
   323                                               # embedding + dropout layer
   324                                               # encoder_outputs: num_batches x 512 q_0 vector
   325                                               # problem_output: max_length x num_batches x hidden_size
   326         9  970273000.0    1e+08      6.9      encoder_outputs, problem_output = models['encoder'](input_var, input_length)
   327                                               # Prepare input and output variables
   328                                           
   329                                               
   330                                               # array of len of numbers that must be copied from the input text
   331         9      75000.0   8333.3      0.0      copy_num_len = [len(_) for _ in num_pos]
   332                                               # max nums to copy
   333         9      24000.0   2666.7      0.0      num_size = max(copy_num_len)
   334                                           
   335                                               # num_batches x num_size x hidden_size that correspond to the embeddings of the numbers
   336         9   15159000.0    2e+06      0.1      all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size, models['encoder'].hidden_size)
   337                                           
   338                                           
   339                                               # index in the language where the special (operators) tokens end and input/output text begins
   340         9      11000.0   1222.2      0.0      num_start = output_lang.num_start
   341                                           
   342         9 3914423000.0    4e+08     27.8      pred_num_equations = models['num_x_predict'](encoder_outputs)
   343                                           
   344         9       2000.0    222.2      0.0      if useCustom:
   345                                                   # node that max(num_equations_per_obs) should be the same as the lenth of vars
   346                                                   xs = models['x_generate'](len(all_vars), encoder_outputs, problem_output)
   347                                               else: 
   348         9       1000.0    111.1      0.0          xs = None 
   349                                           
   350         9       1000.0    111.1      0.0      if useCustom:
   351                                                   qs = models['x_to_q'](encoder_outputs, xs)
   352                                               else:
   353         9          0.0      0.0      0.0          qs = problem_output
   354                                           
   355                                               # do equations one at a time
   356        18     576000.0  32000.0      0.0      for cur_equation in range(max(num_equations_per_obs)):
   357                                                   # select the ith equation in each obs
   358         9    1589000.0 176555.6      0.0          ith_equation_target = deepcopy(target[:, cur_equation, :].transpose(0,1))
   359         9       6000.0    666.7      0.0          if useCustom:
   360                                                       ith_equation_goal = qs[:, cur_equation, :]
   361                                                       node_stacks = [[TreeNode(_)] for _ in ith_equation_goal.split(1, dim=0)]
   362                                                   else:
   363         9       2000.0    222.2      0.0              ith_equation_goal = problem_output
   364         9     348000.0  38666.7      0.0              node_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]
   365                                           
   366         9     204000.0  22666.7      0.0          ith_equation_target_lengths = torch.Tensor(target_length)[:, cur_equation]
   367         9       1000.0    111.1      0.0          ith_equation_num_stacks = []
   368        84      26000.0    309.5      0.0          for stack in nums_stack_batch:
   369        75      63000.0    840.0      0.0              ith_equation_num_stacks.append(stack[cur_equation])
   370                                           
   371                                                   # max_target_length = int(max(ith_equation_target_lengths.tolist()))
   372         9      71000.0   7888.9      0.0          max_target_length = len(ith_equation_target)
   373                                           
   374         9       2000.0    222.2      0.0          all_node_outputs = []
   375         9      29000.0   3222.2      0.0          embeddings_stacks = [[] for _ in range(batch_size)]
   376         9      21000.0   2333.3      0.0          left_childs = [None for _ in range(batch_size)]
   377                                           
   378        96      44000.0    458.3      0.0          for t in range(max_target_length):
   379                                           
   380                                                       # predict gets the encodings and embeddings for the current node 
   381                                                       #   num_score: batch_size x num_length
   382                                                       #       likliehood prediction of each number
   383                                                       #   op: batch_size x num_ops
   384                                                       #       likliehood of the operator tokens
   385                                                       #   current_embeddings: batch_size x 1 x hidden_size
   386                                                       #       goal vector (q) for the current node 
   387                                                       #   current_context: batch_size x 1 x hidden_size
   388                                                       #       context vector (c) for the subtree
   389                                                       #   embedding_weight: batch_size x num_length x hidden_size
   390                                                       #       embeddings of the generate and copy numbers
   391                                           
   392       174  521793000.0    3e+06      3.7              num_score, op, current_embeddings, current_context, current_nums_embeddings = models['predict'](
   393        87      67000.0    770.1      0.0                  node_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, xs, seq_mask, num_mask, useCustom)
   394                                           
   395                                           
   396                                                       # this is mainly what we want to train
   397        87    1054000.0  12114.9      0.0              outputs = torch.cat((op, num_score), 1)
   398        87     132000.0   1517.2      0.0              all_node_outputs.append(outputs)
   399                                           
   400                                                       # target[t] is the equation character at index t for each batch
   401                                                       #    target[t] = 1 x num_batches
   402                                                       # outputs is the strength of operators or a number token
   403                                                       # num_stack_batch is the cooresponding num lists
   404                                                       # num_start is where non-operators begin
   405                                                       # unk is unknown token
   406                                                       # returns
   407                                                       #   for position t in each equation
   408                                                       #       target_t: actual equation value
   409                                                       #       generate_input: equation value if its an operator
   410        87    7357000.0  84563.2      0.1              target_t, generate_input = generate_tree_input(ith_equation_target[t].tolist(), outputs, ith_equation_num_stacks, num_start, unk)
   411        87     916000.0  10528.7      0.0              ith_equation_target[t] = target_t
   412        87      50000.0    574.7      0.0              if USE_CUDA:
   413                                                           generate_input = generate_input.cuda()
   414                                           
   415                                                       # takes:
   416                                                       #     generate a left and right child node with a label
   417                                                       #     current_embeddings: q : batch_size x 1 x hidden_dim
   418                                                       #     generate_input: [operator tokens at position t]
   419                                                       #     current_context: c : batch_size x 1 x hidden_dim
   420                                                       # returns
   421                                                       #     l_child: batch_size x hidden_dim
   422                                                       #          hidden state h_l:
   423                                                       #     r_child: batch_size x hidden_dim
   424                                                       #          hidden state h_r:
   425                                                       #     node_label_ : batch_size x embedding_size 
   426                                                       #          basically the context vector (c)
   427                                                       # the node generation takes the first half of equations (10) and (11) 
   428        87  179364000.0    2e+06      1.3              left_child, right_child, node_label = models['generate'](current_embeddings, generate_input, current_context)
   429        87      85000.0    977.0      0.0              left_childs = []
   430       909   13111000.0  14423.5      0.1              for idx, l, r, node_stack, i, o in zip(range(batch_size), left_child.split(1), right_child.split(1),
   431        87    1200000.0  13793.1      0.0                                                  node_stacks, ith_equation_target[t].tolist(), embeddings_stacks):
   432                                                           # current_token = output_lang.ids_to_tokens([i])
   433                                                           # current_equation = output_lang.ids_to_tokens(target.transpose(0,1)[idx])
   434                                                           #print("at token", current_token, "in", current_equation)
   435                                                           #print("current node_stack length", len(node_stack))
   436                                                           # for 
   437                                                           #   batch_num
   438                                                           #   the left child: h_l 
   439                                                           #   the right child: h_r
   440       735     411000.0    559.2      0.0                  if len(node_stack) != 0:
   441       399    1052000.0   2636.6      0.0                      node = node_stack.pop()
   442                                                               #print("removed last from node_stack, now", len(node_stack), "elems")
   443                                                           else:
   444       336     183000.0    544.6      0.0                      left_childs.append(None)
   445       336      90000.0    267.9      0.0                      continue
   446                                           
   447                                                           # i is the num in language of where that specific language token is
   448                                                           # if i is an operator
   449       399     150000.0    375.9      0.0                  if i < num_start:
   450                                                               #print(current_token, "is an operator, making a left and right node")
   451                                                               # make a left and right tree node
   452       162     312000.0   1925.9      0.0                      node_stack.append(TreeNode(r))
   453       162     392000.0   2419.8      0.0                      node_stack.append(TreeNode(l, left_flag=True))
   454                                                               # save the embedding of the operator 
   455                                                               # terminal means a leaf node
   456       162    1614000.0   9963.0      0.0                      o.append(TreeEmbedding(node_label[idx].unsqueeze(0), False))
   457                                                               #print("saving node embedding to o (non terminal node), and r, and l to node_stack. o now of size", len(o), "node_stack of size", len(node_stack))
   458                                                           else:
   459                                                               #print(current_token, "is not an operator")
   460                                                               # otherwise its either a number from the input equation or a copy number
   461                                                               # we have a list (o) of the current nodes in the tree
   462                                                               # if we have a leaf node at the top of the stack, get it.
   463                                                               # next element in the stack must be an operator, so get it 
   464                                                               # and combine the new node, operator, and other element
   465                                           
   466                                                               # current_nums_embedding: batch_size x num_length x hidden_size
   467                                                               # current_num = num_embedding of the number selected
   468       237    3311000.0  13970.5      0.0                      current_num = current_nums_embeddings[idx, i - num_start].unsqueeze(0)
   469                                                               # while there are tokens in the embedding stack and the last element IS a leaf node
   470       399     645000.0   1616.5      0.0                      while len(o) > 0 and o[-1].terminal:
   471                                                                   #print("terminal element in o, getting terminal element and operator, and merging")
   472                                                                   # get the two elements from it
   473       162    1220000.0   7530.9      0.0                          sub_stree = o.pop()
   474       162     331000.0   2043.2      0.0                          op = o.pop()
   475                                                                   # contains equation (13)
   476                                                                   # this combines a left and right tree along with a node
   477       162   58981000.0 364080.2      0.4                          current_num = models['merge'](op.embedding, sub_stree.embedding, current_num)
   478                                                                   #print('merged. o now of size', len(o))
   479                                                               # then re-add the node back to the stack
   480                                                               #print("adding current_num to o (terminal node)")
   481       237     678000.0   2860.8      0.0                      o.append(TreeEmbedding(current_num, True))
   482       399     317000.0    794.5      0.0                  if len(o) > 0 and o[-1].terminal:
   483                                                               #print("terminal element in o, adding to left child")
   484                                                               # left_childs is a running vector of the sub tree embeddings "t" 
   485                                                               # need this for generation of the right q
   486       237     189000.0    797.5      0.0                      left_childs.append(o[-1].embedding)
   487                                                           else:
   488       162      86000.0    530.9      0.0                      left_childs.append(None)
   489                                           
   490                                                   # all_leafs = torch.stack(all_leafs, dim=1)  # B x S x 2
   491                                                   
   492                                                   # all_node_outputs:  for each token in the equation:
   493                                                   #   the current scoring of nums for each batch
   494                                                   # 
   495                                                   # transform to 
   496                                                   # all_node_outputs2: for each batch:
   497                                                   #   the current scoring of nums for each token in equation
   498                                                   # = batch_size x max_len x num_nums
   499         9     212000.0  23555.6      0.0          all_node_outputs2 = torch.stack(all_node_outputs, dim=1)  # B x S x N
   500                                           
   501         9      61000.0   6777.8      0.0          ith_equation_target = ith_equation_target.transpose(0, 1).contiguous()
   502         9       5000.0    555.6      0.0          if USE_CUDA:
   503                                                       # all_leafs = all_leafs.cuda()
   504                                                       all_node_outputs2 = all_node_outputs2.cuda()
   505                                                       ith_equation_target = ith_equation_target.cuda()
   506                                           
   507                                                   # for batch in target:
   508                                                   #     print([output_lang.index2word[_] for _ in batch])
   509                                                   #print('done equation')
   510                                                   # loss = masked_cross_entropy(all_node_outputs2, target, target_length)
   511         9    2151000.0 239000.0      0.0          current_equation_loss = torch.nn.CrossEntropyLoss(reduction="none")(all_node_outputs2.view(-1, all_node_outputs2.size(2)), ith_equation_target.view(-1).to(device)).mean()
   512         9       4000.0    444.4      0.0          same = 0
   513         9       2000.0    222.2      0.0          lengths = 0
   514                                                   # print(f'Equation {cur_equation}')
   515        84     298000.0   3547.6      0.0          for i, batch in enumerate(all_node_outputs2):
   516        75     265000.0   3533.3      0.0              vals = []
   517        75     374000.0   4986.7      0.0              equ_length = int(ith_equation_target_lengths[i].item())
   518       538    1545000.0   2871.7      0.0              for j, probs in enumerate(batch):
   519       527    2029000.0   3850.1      0.0                  max_val = torch.argmax(probs)
   520       527     254000.0    482.0      0.0                  vals.append(max_val)
   521       527     148000.0    280.8      0.0                  lengths += 1
   522       527     136000.0    258.1      0.0                  if j > equ_length:
   523        64     236000.0   3687.5      0.0                      break
   524       463    3368000.0   7274.3      0.0                  if max_val == ith_equation_target[i][j]:
   525       128      56000.0    437.5      0.0                      same += 1
   526                                                       # print(f"        prediction: {[output_lang.index2word[_] for _ in vals[0:equ_length]]}")
   527                                                       # print(f"        actual: {[output_lang.index2word[_] for _ in ith_equation_target[i][0:equ_length]]}")
   528         9       5000.0    555.6      0.0          if total_loss != None:
   529                                                       total_loss += current_equation_loss
   530                                                   else:
   531         9       3000.0    333.3      0.0              total_loss = current_equation_loss
   532         9      19000.0   2111.1      0.0          total_acc += [same/lengths]
   533                                               
   534                                               # add the loss of number equations
   535         9     971000.0 107888.9      0.0      num_x_loss = torch.nn.CrossEntropyLoss()(pred_num_equations, num_equations_per_obs.to(device))
   536                                                       
   537         9     159000.0  17666.7      0.0      total_loss += num_x_loss
   538         9 8384136000.0    9e+08     59.5      total_loss.backward()
   539                                           
   540                                               # Update parameters with optimizers
   541         9     198000.0  22000.0      0.0      return total_loss.item(), sum(total_acc)/len(total_acc)

 14.10 seconds - /Users/home/school/thesis/math_seq2tree/src/train_and_evaluate.py:259 - train_tree
